{"nbformat_minor": 0, "cells": [{"execution_count": 1, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": ["%matplotlib inline\n", "%load_ext autoreload\n", "%autoreload 2"]}, {"execution_count": 2, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": ["from RRNN import RRNN\n", "from NeuroTools.signals.spikes import SpikeTrain\n", "import numpy as np"]}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": ["net = RRNN(recurrent=True, ring=False)\n", "df, spikesE, spikesI = net.model()\n", "\n", "for spiketrain in spikesE.spiketrains:\n", "    print 'Fano factor : {0}'.format(SpikeTrain(spiketrain).fano_factor_isi())"]}, {"source": ["Fano factor forces us to manage case of silent neurons. Another possible approach is to compute an equivalent index using cv of collapsed activity of population neurons over cvs mean."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": ["A = np.array([[1, 2, 6], [1, 3, 6]])"]}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": ["print (A.flatten())"]}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": ["print (np.unique(A))"]}, {"source": ["np.unique() is apparently the function to use with our spiketrains"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": ["tab = np.array(spikesE.spiketrains)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": ["print(tab) "]}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": ["tab = np.array([])\n", "for st in spikesE.spiketrains:\n", "    tab = np.append(tab, np.array(st), axis=0)\n", "print tab"]}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": ["tab2 = np.unique(tab)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"scrolled": true, "collapsed": false}, "outputs": [], "source": ["print (tab2)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": ["cv_collapsedspikes = SpikeTrain(tab2).cv_isi()"]}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": ["print cv_collapsedspikes"]}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": ["all_CVs = np.array([])\n", "for st in spikesE.spiketrains :\n", "    all_CVs = np.append(all_CVs, SpikeTrain(np.array(st)).cv_isi())\n", "meanCv = np.nanmean(all_CVs)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": ["print(meanCv)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": ["fakefano = cv_collapsedspikes/meanCv"]}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": ["print fakefano"]}, {"source": ["Now a little test with poisson process"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": ["import pyNN.nest as sim"]}, {"execution_count": 4, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": ["sim.setup(timestep=0.1)\n", "\n", "poisson = sim.Population(100, sim.SpikeSourcePoisson(rate=10))\n", "poisson.record('spikes')\n", "\n", "sim.run(1000)\n", "poisson_spikes = poisson.get_data().segments[0]\n", "\n", "tab = np.array([])\n", "for st in poisson_spikes.spiketrains:\n", "    tab = np.append(tab, np.array(st), axis=0)\n", "\n", "cv_collapsed = SpikeTrain(np.unique(tab)).cv_isi()\n", "\n", "all_CVs = np.array([])\n", "for st in poisson_spikes.spiketrains :\n", "    all_CVs = np.append(all_CVs, SpikeTrain(np.array(st)).cv_isi())\n", "\n", "meanCv = np.nanmean(all_CVs)\n", "\n", "fakefano = cv_collapsed/meanCv\n", "\n", "print fakefano"]}, {"execution_count": 5, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": ["np.array(poisson_spikes.spiketrains[1])"]}, {"source": ["## pyspike\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["%%bash\n", "cd ~/Downloads\n", "git clone https://github.com/mariomulansky/PySpike.git\n", "cd PySpike\n", "python setup.py build_ext --inplace\n", "python setup.py build\n", "python setup.py install"], "cell_type": "raw", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": ["def reliability(spike_data, method='purpura'):\n", "    spike_trains = []    \n", "    for st_ in spike_data:\n", "        spike_trains.append(st_)\n", "\n", "    if method=='pyspike':\n", "        import pyspike as spk\n", "        spike_trains = [ spk.SpikeTrain(st_, edges=(0, params['block_duration'])) for st_ in spike_trains]   # spike_trains.append(spk.SpikeTrain(st_, edges=(0, params['block_duration'])))\n", "\n", "        #from pyspike import spike_sync_profile as profile # , max_tau=1000\n", "        from pyspike import spike_profile as profile\n", "        spk.disable_backend_warning = True\n", "        avrg_profile = profile(spike_trains).avrg()\n", "\n", "    elif method=='purpura':\n", "        import elephant\n", "        from neo.core import SpikeTrain\n", "        from quantities import s\n", "\n", "        #spike_trains = [ SpikeTrain(st_*s, t_stop=1*s) for st_ in spike_trains] \n", "        #for st_ in spike_data[:8]: # HACK to remove the last trials which seem weak\n", "        #    spike_trains.append(SpikeTrain(st_*s, t_stop=params['block_duration']*s))\n", "        D = elephant.spike_train_dissimilarity.victor_purpura_dist(spike_trains)\n", "        avrg_profile = D.mean()\n", "\n", "    elif method=='van_rossum':\n", "        import elephant\n", "        from neo.core import SpikeTrain\n", "        from quantities import s\n", "\n", "        spike_trains = [ SpikeTrain(st_*s, t_stop=params['block_duration']*s) for st_ in spike_trains] \n", "        #for st_ in spike_data[:8]: # HACK to remove the last trials which seem weak\n", "        #    spike_trains.append(SpikeTrain(st_*s, t_stop=params['block_duration']*s))\n", "        D = elephant.spike_train_dissimilarity.van_rossum_dist(spike_trains, tau=np.array(1.0) * s)\n", "        avrg_profile = D.mean()\n", "\n", "    return avrg_profile\n", "            \n", "\n", "print (\"reliability = \" + \"{0:.4f}\".format(reliability(poisson_spikes.spiketrains)))\n"]}, {"execution_count": 9, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": ["net = RRNN(recurrent=True, ring=False)\n", "df, spikesE, spikesI = net.model()"]}, {"execution_count": 10, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": ["print (\"reliability = \" + \"{0:.4f}\".format(reliability(spikesE.spiketrains)))"]}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "2.7.12", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}, "nbformat": 4}