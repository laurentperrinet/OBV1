{"nbformat_minor": 0, "nbformat": 4, "cells": [{"source": ["# Introduction\n", "\n", "## Motivation\n", "\n", "Une entit\u00e9 ne peut \u00eatre vivante si elle n'interagit pas avec son environnement. La vie, r\u00e9duit \u00e0 son plus simple appareil, comporte du code g\u00e9n\u00e9tique. Pour se perp\u00e9tuer, ce code g\u00e9n\u00e9tique doit permettre de produire des mol\u00e9cules qui vont former le mat\u00e9riel n\u00e9cessaire \u00e0 sa protection, \u00e0 son alimentation et enfin \u00e0 sa reproduction. La cellule remplit ces r\u00f4les, aussi, sa membrane et les prot\u00e9ines qui la constituent lui permettent de jouer un r\u00f4le d'interface entre le code g\u00e9n\u00e9tique et l'environnement. Il apparait que la notion d'interface soit importante dans le vivant. Ainsi, l'interface se conserve et se sophistique, au fil de nombreuses mutations du code g\u00e9n\u00e9tique et de la s\u00e9lection naturelle. Cette derni\u00e8re va permettre l'\u00e9mergence d'interfaces de plus en plus \u00e9labor\u00e9es, qui vont offrir de multiples mani\u00e8res de filtrer l'environnement et des moyens d'explorer celui-ci.\n", "\n", "Avec l'apparition des organismes pluricellulaires et des moyens de communications entre les cellules, les cellules se sp\u00e9cialisent, s'assemblent et constituent des tissus cellulaires, des organes et des syst\u00e8mes qui vont permettre \u00e0 l'ensemble de l'organisme d'interagir, d'une mani\u00e8re sp\u00e9cifique au syst\u00e8me consid\u00e9r\u00e9, avec l'environnement. Le syst\u00e8me nerveux en est un exemple. En effet, celui-ci permet de traiter des informations externes ou internes, et une partie de ce traitement comprend diverses fonctions que l'on regroupe dans le concept de perception.\n", "\n", "Selon l'approche neurophysiologique de la vision, la perception visuelle est construite sur la photosensibilit\u00e9 de certaines cellules, les c\u00f4nes et les b\u00e2tonnets. Ces cellules et d'autres comme, par exemple, les cellules ganglionnaires, forment la r\u00e9tine tapissant le fond de l'oeil o\u00f9, \u00e0 tout instant, une image de l'environnement se projette. Les cellules photosensibles vont permettre d'encoder les variations locales de luminosit\u00e9 composant l'image en variations de potentiel membranaire. Ces variations de potentiel vont ensuite \u00eatre cod\u00e9es en influx nerveux qui vont \u00eatre transmis dans le r\u00e9seau r\u00e9tinien pour y \u00eatre transform\u00e9s. Les informations visuelles arrivent alors au niveau du cortex visuel primaire via les entr\u00e9es thalamiques. L'organisation de ce cortex est r\u00e9tinotopique. Ainsi, l'activit\u00e9 du cortex visuel primaire repr\u00e9sente l'espace visuel, compos\u00e9 d'\u00e9l\u00e9ments locaux comme, par exemple, les bords orient\u00e9s et les couleurs. Une question se pose alors lorsqu'on cherche \u00e0 associer ce substrat biologique \u00e0 la perception visuelle : comment le syst\u00e8me nerveux central r\u00e9alise l'int\u00e9gration de ces \u00e9l\u00e9ments locaux afin de constuire un percept global ?\n", "\n", "Afin de r\u00e9pondre \u00e0 cette large probl\u00e9matique, il convient de s'int\u00e9resser aux divers constituants de l'image composante par composante. Ainsi, le travail qui vous est pr\u00e9sent\u00e9 est d\u00e9di\u00e9 \u00e0 l'\u00e9tude de la d\u00e9tection d'orientations. Il s'incrit dans un projet interdisciplinaire port\u00e9 par l'\u00e9quipe InViBe au sein de l'Institut de Neurosciences de la Timone. Ce projet pr\u00e9voit, outre diverses exp\u00e9rimentations chez l'animal, une approche computationnelle des m\u00e9canismes impliqu\u00e9s dans cette d\u00e9tection d'orientations, qui constitue l'objet de ce travail.\n", "\n", "## Contexte scientifique\n", "\n", "### Organisation des orientations\n", "Des \u00e9lectrophysiologistes tels que Hubel et Wiesel ont mis en \u00e9vidence, chez le chat, que des colonnes corticales du cortex visuel primaire ont une sensibilit\u00e9 pr\u00e9f\u00e9rentielle \u00e0 une orientation possible de barres de contraste<cite data-cite=\"Hubel\">(Hubel, Wiesel, 1962)</cite>. L'avanc\u00e9e des \u00e9tudes sur la s\u00e9lectivit\u00e9 \u00e0 l'orientation des neurones corticaux, montrent que le cortex visuel primaire des mammif\u00e8res carnivores et des primates est comme une carte, o\u00f9 les neurones de m\u00eame s\u00e9lectivit\u00e9 \u00e0 l'orientation sont regroup\u00e9s en domaines d'iso-orientation. L'organisation particuli\u00e8re de ces domaines ou \u00eelots donne lieu \u00e0 des propri\u00e9t\u00e9s remarquables. En effet, il existe diff\u00e9rents voisinages d'un neurone se trouvant dans le cortex visuel primaire. Si celui-ci se trouve \u00e0 l'int\u00e9rieur des \u00eelots, il est \u00e0 proximit\u00e9 de neurones de m\u00eame, ou proche, s\u00e9lectivit\u00e9 \u00e0 l'orientation. Si, en revanche, toutes les orientations sont cod\u00e9es par son voisinage, alors il est \u00e0 l'int\u00e9rieur des fractures (ou pinwheels), o\u00f9 la s\u00e9lectivit\u00e9 \u00e0 l'orientation varie rapidement <cite data-cite=\"ohki\">(Ohki et al., 2006)</cite> <cite data-cite=\"grinvald\">(Bonhoeffer, Grinvald 1991)</cite>. Dans le cortex visuel primaire du rongeur, il n'existe pas de domaines d'iso-orientation, le voisinage d'un neurone quelconque est alors de la deuxi\u00e8me esp\u00e8ce cit\u00e9e.\n", "\n", "### R\u00e9ponse \u00e0 une orientation\n", "\n", "De telles organisations suscitent des hypoth\u00e8ses quant \u00e0 l'int\u00e9gration des informations sur l'orientation au sein des colonnes corticales du cortex visuel primaire.\n", "Une d'entre elles postule que la probablit\u00e9 de connexion entre les neurones du cortex visuel primaire est exclusivement d\u00e9pendante de leur distance anatomique <cite data-cite=\"das\">(Das, Gilbert 1999)</cite>. Ce qui signifie que, dans le cas admis o\u00f9 il existe des connexions r\u00e9currentes et lat\u00e9rales au niveau cortical, les neurones \u00e0 l'int\u00e9rieur des domaines d'iso-orientation int\u00e8grent des informations provenant de neurones de m\u00eame pr\u00e9f\u00e9rence \u00e0 l'orientation. Ainsi, la r\u00e9ponse de ces neurones est fortement s\u00e9lective et est robuste \u00e0 la richesse en orientations d'un stimulus. Cela veut dire \u00e9galement qu'\u00e0 proximit\u00e9 des fractures, et dans le cortex visuel primaire du rat, les neurones devraient avoir une faible s\u00e9lectivit\u00e9 \u00e0 l'orientation car ils int\u00e8grent les informations provenant de neurones s\u00e9lectifs \u00e0 diff\u00e9rentes orientations. Ce n'est pourtant pas ce qui est montr\u00e9 exp\u00e9rimentalement. Une \u00e9tude explique alors ce paradoxe. En effet, il a \u00e9t\u00e9 th\u00e9oriquement d\u00e9montr\u00e9 que la r\u00e9ponse de ces neurones, suppos\u00e9s peu s\u00e9lectifs, peut \u00eatre plus s\u00e9lective \u00e0 l'orientation si le r\u00e9seau du cortex visuel primaire est dans un \u00e9tat balanc\u00e9 entre l'excitation et l'inhibition <cite data-cite=\"HanselVan\">(Hansel, Van Vreeswijk 2012)</cite>.\n", "\n", "Nous nous proposons donc d'\u00e9tudier la r\u00e9ponse d'un r\u00e9seau de neurones artificiel, un ring, \u00e0 diff\u00e9rents stimuli visuels, des motion clouds <cite data-cite=\"Leon12\">(Leon)</cite>, dont nous ferons alors varier la richesse en orientations. En effet, le contenu en orientations de chaque stimulus peut \u00eatre d\u00e9fini de fa\u00e7on quantitative en modulant une certaine valeur de bandwidth $B_\\theta$ caract\u00e9risant l'entr\u00e9e visuelle. Le but est d'impl\u00e9menter le r\u00e9seau, de fa\u00e7on \u00e0 ce qu'il poss\u00e8de des propri\u00e9t\u00e9s similaires \u00e0 celles \u00e9voqu\u00e9es plus haut concernant le cortex visuel primaire. Nous comparerons alors la r\u00e9ponse de ce r\u00e9seau \u00e0 des donn\u00e9es physiologiques.\n", "\n", "\n", "![R\u00e9seau de neurones organis\u00e9 en \"ring\".](/tmp/ring_model.png)"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Plan du m\u00e9moire\n", "\n", "Dans un premier temps, nous traiterons des r\u00e9seaux de neurones artificiels, de leur impl\u00e9mentation ainsi que de l'\u00e9tude de leurs comportements. Nous y \u00e9tudierons notamment le r\u00e9seau r\u00e9current al\u00e9atoire et tenterons de mettre en \u00e9vidence certains de ces \u00e9tats, notamment l'\u00e9tat balanc\u00e9. Ensuite, nous \u00e9tudierons le ring, un r\u00e9seau r\u00e9current al\u00e9atoire disposant d'une certaine organisation de neurones. Nous tenterons alors de montrer que ce r\u00e9seau est un mod\u00e8le satisfaisant du codage de l'orientation au sein du cortex visuel primaire."], "cell_type": "markdown", "metadata": {}}, {"source": ["# Le r\u00e9seau de neurones artificiels\n", "\n", "Dans ce chapitre, nous d\u00e9velopperons diff\u00e9rents aspects de l'impl\u00e9mentation en Python de r\u00e9seaux de neurones artificiels, en allant du plus simple au plus complexe. Nous commencerons donc par introduire le neurone \"int\u00e8gre et d\u00e9charge\" et traiterons de son impl\u00e9mentation dans un r\u00e9seau simple. Cette impl\u00e9mentation nous permettra de tester et comparer diff\u00e9rents simulateurs de r\u00e9seaux neuronaux. Puis, nous traiterons du 'random recurrent neural network' (RRNN) et de l'exploration de ses r\u00e9gimes dynamiques. Enfin, nous \u00e9tudierons diff\u00e9rents \u00e9tats du RRNN, notamment de l'\u00e9tat dit \"balanc\u00e9\".\n"], "cell_type": "markdown", "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "deletable": true}}, {"source": ["## Le neurone int\u00e8gre et d\u00e9charge\n", "        \n", "Le neurone int\u00e8gre et d\u00e9charge, ou \"integrate and fire\", est un mod\u00e8le r\u00e9pandu du neurone biologique lorsque l'on d\u00e9sire simuler le fonctionnement de r\u00e9seaux de neurones. Contrairement \u00e0 d'autres mod\u00e8les plus r\u00e9alistes comme celui d'Hodgkin et Huxley <cite data-cite=\"Hodgkin\">(Hodgkin, Huxley 1952)</cite>, il n\u00e9glige l'effet des courants ioniques sur le potentiel membranaire. Mais cette approximation permet de gagner en temps de calcul, en facteur d'ordre 2 <cite data-cite=\"Izhi\">(Izhikevitch, 2004)</cite> et offre la possibilit\u00e9 de simuler un r\u00e9seau de neurones en un temps raisonnable. Sa variante la plus r\u00e9pandue est le neurone \"leaky integrate and fire\" (LIF), qui permet de prendre en consid\u00e9ration des courants de fuite, qui sont regroup\u00e9s en un terme de r\u00e9sistance membranaire.\n", "\n", "La dynamique du potentiel de membrane $V$ d'un neurone LIF est d\u00e9finie par :\n", "$$\n", "\\tau_m \\frac{d}{dt}V = E_L - V + R_mI_e\n", "$$\n", "\n", "avec\n", "\n", "- $\\tau_m$ : constante de temps membranaire\n", "- $E_L$ : potentiel de repos \n", "- $R_m$ : r\u00e9sistance membranaire\n", "- $I_e$ : courant entrant\n", "\n", "Un neurone LIF g\u00e9n\u00e8re un potentiel d'action quand $V$ d\u00e9passe une valeur seuil $V_{threshold}$. Une fois ce seuil d\u00e9pass\u00e9 $V$ est r\u00e9initialis\u00e9 \u00e0 une valeur $V_{reset}$.\n", "\n", "Diff\u00e9rents mod\u00e8les de neurones LIF ont \u00e9t\u00e9 d\u00e9velopp\u00e9s. Certains de ces mod\u00e8les tentent de concilier l'efficacit\u00e9 en temps de calcul du neurone LIF et le bior\u00e9alisme du mod\u00e8le de Hodgkin et Huxley. Ainsi, il est possible de distinguer les neurones LIF conductance-based (COBA) des neurones LIF current-based (CUBA). Le mod\u00e8le CUBA consid\u00e8re les entr\u00e9es synaptiques comme des courants entrants qui vont, comme l'\u00e9quation au dessus l'indique, faire \u00e9voluer le potentiel membranaire. Le mod\u00e8le COBA, en revanche, traduit les entr\u00e9es synaptiques par des changements de conductance, de telle sorte que des entr\u00e9es vont amplifier les effets d'autres entr\u00e9es sur le potentiel de membrane. Le mod\u00e8le COBA est donc un peu plus bior\u00e9aliste que le mod\u00e8le CUBA mais aussi plus complexe \u00e0 \u00e9tudier <cite data-cite=\"hansel1995synchrony\">(Hansel et al., 1995)</cite>.\n", "\n", "Ce projet porte sur la mod\u00e9lisation du cortex visuel primaire et nous pousse donc \u00e0 choisir notre mod\u00e8le neuronal parmi ceux disposant d'un minimum de r\u00e9alisme. C'est pourquoi notre choix se porte sur le mod\u00e8le COBA."], "cell_type": "markdown", "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "deletable": true}}, {"source": ["### Impl\u00e9mentation sous BRIAN\n", "\n", "Diff\u00e9rents simulateurs sont test\u00e9s afin d'\u00e9valuer leurs souplesse d'utilisation ainsi que leur efficacit\u00e9.\n", "Nous commen\u00e7ons donc par tester Brian. (http://brian-simulator.org). La particularit\u00e9 de Brian est qu'il permet \u00e0 son utilisateur de cr\u00e9er un mod\u00e8le en rentrant l'\u00e9quation diff\u00e9rentielle de sa dynamique. Il permet donc une souplesse maximale quant au choix du mod\u00e8le neuronal, tout en prenant en charge la mise en r\u00e9seau des neurones. Afin de se familiariser avec cet outil, nous impl\u00e9mentons un simple neurone int\u00e8gre et d\u00e9charge. (voir  https://brian.readthedocs.org/en/latest/tutorial1_basic_concepts.html#tutorial-1-basic-concepts)\n"], "cell_type": "markdown", "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "deletable": true}}, {"execution_count": 2, "cell_type": "code", "source": ["from brian2 import *\n", "start_scope()\n", "\n", "eqs = '''\n", "dv/dt = (I-v)/tau : 1\n", "I : 1\n", "tau : second\n", "'''\n", "G = NeuronGroup(3, eqs, threshold='v>1', reset='v = 0', method='linear')\n", "G.I = [2, 0, 0]\n", "G.tau = [10, 100, 100]*ms\n", "\n", "S = Synapses(G, G, 'w : 1', on_pre='v_post += w')\n", "S.connect(i=0, j=[1, 2])\n", "S.w = 'j*0.2'\n", "S.delay = 'j*2*ms'\n", "\n", "M = StateMonitor(G, 'v', record=True)\n", "\n", "run(50*ms)\n", "\n", "plot(M.t/ms, M.v[0], '-b', label='Neuron 0')\n", "plot(M.t/ms, M.v[1], '-g', lw=2, label='Neuron 1')\n", "plot(M.t/ms, M.v[2], '-r', lw=2, label='Neuron 1')\n", "xlabel('Time (ms)')\n", "ylabel('v')\n", "legend(loc='best');"], "outputs": [], "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "collapsed": false, "deletable": true}}, {"source": ["### Simulation d'un r\u00e9seau de neurones avec BRIAN (r\u00e9seau CUBA)\n", "\n", "Nous impl\u00e9mentons \u00e9galement un r\u00e9seau de neurones CUBA."], "cell_type": "markdown", "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "collapsed": true, "deletable": true}}, {"source": ["https://brian.readthedocs.org/en/latest/tutorial2_connections.html#tutorial-2-connections"], "cell_type": "markdown", "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "deletable": true}}, {"execution_count": 2, "cell_type": "code", "source": ["from brian2 import *\n", "\n", "taum = 20*ms\n", "taue = 5*ms\n", "taui = 10*ms\n", "Vt = -50*mV\n", "Vr = -60*mV\n", "El = -49*mV\n", "\n", "eqs = '''\n", "dv/dt  = (ge+gi-(v-El))/taum : volt (unless refractory)\n", "dge/dt = -ge/taue : volt\n", "dgi/dt = -gi/taui : volt\n", "'''\n", "\n", "P = NeuronGroup(4000, eqs, threshold='v>Vt', reset='v = Vr', refractory=5*ms,\n", "                method='linear')\n", "P.v = 'Vr + rand() * (Vt - Vr)'\n", "P.ge = 0*mV\n", "P.gi = 0*mV\n", "\n", "we = (60*0.27/10)*mV # excitatory synaptic weight (voltage)\n", "wi = (-20*4.5/10)*mV # inhibitory synaptic weight\n", "Ce = Synapses(P, P, on_pre='ge += we')\n", "Ci = Synapses(P, P, on_pre='gi += wi')\n", "Ce.connect('i<3200', p=0.02)\n", "Ci.connect('i>=3200', p=0.02)\n", "\n", "s_mon = SpikeMonitor(P)\n", "\n", "run(1 * second)\n", "\n", "plot(s_mon.t/ms, s_mon.i, '.k')\n", "xlabel('Time (ms)')\n", "ylabel('Neuron index')\n", "show()"], "outputs": [], "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "collapsed": false, "deletable": true}}, {"execution_count": 3, "cell_type": "code", "source": ["from brian2tools import plot_raster\n", "help(plot_raster)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": ["plot_raster(s_mon.i, s_mon.t, time_unit=second, marker=',', color='k');"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Impl\u00e9mentation sous Nest\n", "\n", "Nest (http://www.nest-simulator.org/) est un autre simulateur de r\u00e9seaux de neurones sp\u00e9cialis\u00e9 dans la mod\u00e9lisation de larges r\u00e9seaux de neurones simplifi\u00e9s. Celui-ci, en revanche, ne permet pas de d\u00e9finir un mod\u00e8le neuronal fin en saisissant explicitement des \u00e9quations diff\u00e9rentielles. Le choix est fait d'optimiser plut\u00f4t ces mod\u00e8les en les compilant de fa\u00e7on efficace. Comme avec Brian, nous impl\u00e9mentons une simulation d'un simple mod\u00e8le \"integrate and fire\" en suivant la m\u00eame formalisation."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": ["import nest\n", "import matplotlib.pyplot as plt\n", "neuron = nest.Create(\"iaf_neuron\")\n", "\n", "nest.GetStatus(neuron)\n", "\n", "nest.GetStatus(neuron, \"I_e\")\n", "nest.GetStatus(neuron, [\"V_reset\", \"V_th\"])\n", "\n", "nest.SetStatus(neuron, {\"I_e\": 376.0})\n", "\n", "nest.GetStatus(neuron, \"I_e\")"], "outputs": [], "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "collapsed": false, "deletable": true}}, {"execution_count": 3, "cell_type": "code", "source": ["spikedetector = nest.Create(\"spike_detector\", \n", "                            params={\"withgid\":True, \"withtime\":True})\n", "\n", "multimeter = nest.Create(\"multimeter\")\n", "nest.SetStatus(multimeter, {\"withtime\":True, \"record_from\":[\"V_m\"]})\n", "\n", "noise = nest.Create(\"poisson_generator\", 2)\n", "nest.SetStatus(noise, [{\"rate\": 8000.0}, {\"rate\": 15000.0}])\n", "nest.SetStatus(neuron, {\"I_e\": 0.0})\n", "\n", "syn_dict_ex = {\"weight\": 1.2}\n", "syn_dict_in = {\"weight\": -2.0}\n", "nest.Connect([noise[0]], neuron, syn_spec=syn_dict_ex)\n", "nest.Connect([noise[1]], neuron, syn_spec=syn_dict_in)\n", "\n", "nest.Connect(multimeter, neuron)\n", "nest.Connect(neuron, spikedetector)"], "outputs": [], "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "collapsed": true, "deletable": true}}, {"execution_count": 4, "cell_type": "code", "source": ["%%timeit\n", "nest.Simulate(200.0)"], "outputs": [], "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "collapsed": false, "deletable": true}}, {"execution_count": 5, "cell_type": "code", "source": ["dmm = nest.GetStatus(multimeter)[0]\n", "Vms = dmm[\"events\"][\"V_m\"]\n", "ts = dmm[\"events\"][\"times\"]\n", "\n", "plt.figure(figsize=(15,5))\n", "plt.plot(ts, Vms)"], "outputs": [], "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "collapsed": false, "deletable": true}}, {"execution_count": 6, "cell_type": "code", "source": ["dSD = nest.GetStatus(spikedetector, keys='events')[0]\n", "evs = dSD[\"senders\"]\n", "ts = dSD[\"times\"]\n", "plt.figure(figsize=(15,5))\n", "plt.plot(ts, evs, \".\")\n", "plt.show()"], "outputs": [], "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "collapsed": false, "deletable": true}}, {"source": ["### Impl\u00e9mentation sous PyNN\n", "\n", "Nous avons vu qu'une impl\u00e9mentation d'un m\u00eame mod\u00e8le peut radicalement changer de forme selon le simulateur utilis\u00e9. Il peut \u00eatre utile, afin de ne pas passer trop de temps sur l'apprentissage des diff\u00e9rents simulateurs, de trouver un moyen d'harmoniser l'\u00e9criture des mod\u00e8les. PyNN (http://neuralensemble.org/PyNN) n'est pas un simulateur mais une API, une interface de description (i.e. programmation), qui permet de r\u00e9aliser des simulations pouvant s'ex\u00e9cuter sur diff\u00e9rents simulateurs, comme Nest ou Brian, sans changer le code d'impl\u00e9mentation. Elle permet \u00e9galement de g\u00e9n\u00e9rer divers graphiques ais\u00e9ment <cite data-cite=\"davis\">(Davison et al., 2008)</cite>.\n", "\n", "C'est avec cette interface que nous allons construire des r\u00e9seaux neuronaux et programmer diff\u00e9rents outils pour leur \u00e9tude. Nous impl\u00e9mentons tout d'abord un r\u00e9seau tr\u00e8s simple afin de comparer l'efficacit\u00e9 de Nest et de Brian. Ces simulations permettent de v\u00e9rifier que les impl\u00e9mentations de NEST et Brian fournissent des r\u00e9sultats quasi-identiques au niveau de l'impl\u00e9mentation des \u00e9quations diff\u00e9rentielles des mod\u00e8les neuronaux. Il s'av\u00e8re aussi que la simulation avec Nest se fait en un temps plus court que celle effectu\u00e9e avec Brian. C'est pourquoi d\u00e9sormais nous utiliserons Nest pour la simulation de r\u00e9seaux de neurones."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": ["import pyNN.nest as sim\n", "import numpy\n", "\n", "from pyNN.utility import get_simulator, init_logging, normalized_filename\n", "from pyNN.parameters import Sequence\n", "from pyNN.random import RandomDistribution as rnd\n", "from pyNN.utility.plotting import Figure, Panel\n", "\n", "\n", "# === Define parameters ========================================================\n", "\n", "n = 10      # Number of cells\n", "w = 0.2  # synaptic weight (\u00b5S)\n", "coba_params = {\n", "    'tau_m'      : 20.0,   # (ms)\n", "    'tau_syn_E'  : 2.0,    # (ms)\n", "    'tau_syn_I'  : 4.0,    # (ms)\n", "    'e_rev_E'    : 0.0,    # (mV)\n", "    'e_rev_I'    : -70.0,  # (mV)\n", "    'tau_refrac' : 2.0,    # (ms)\n", "    'v_rest'     : -60.0,  # (mV)\n", "    'v_reset'    : -70.0,  # (mV)\n", "    'v_thresh'   : -50.0,  # (mV)\n", "    'cm'         : 0.5}    # (nF)\n", "\n", "cuba_params = {\n", "    'tau_m'      : 20.0,   # (ms)\n", "    'tau_syn_E'  : 2.0,    # (ms)\n", "    'tau_syn_I'  : 4.0,    # (ms)\n", "    'tau_refrac' : 2.0,    # (ms)\n", "    'v_rest'     : -60.0,  # (mV)\n", "    'v_reset'    : -70.0,  # (mV)\n", "    'v_thresh'   : -50.0,  # (mV)\n", "    'cm'         : 0.5,    # (nF)\n", "    'i_offset'   : 0.0}\n", "dt         = 0.1           # (ms)\n", "syn_delay  = 1.0           # (ms)\n", "input_rate = 10.0         # (Hz)\n", "simtime    = 1000.0        # (ms)\n", "\n", "# === Build the network ========================================================\n", "\n", "sim.setup()\n", "\n", "coba = sim.Population(n, sim.IF_cond_alpha(**coba_params),\n", "                       initial_values={'v': rnd('uniform', (-60.0, -50.0))},\n", "                       label=\"coba\")\n", "\n", "cuba = sim.Population(n, sim.IF_curr_alpha(**cuba_params),\n", "                       initial_values={'v': rnd('uniform', (-60.0, -50.0))},\n", "                       label=\"cuba\")\n", "\n", "\n", "number = int(2*simtime*input_rate/1000.0)\n", "numpy.random.seed(26278342)\n", "\n", "\n", "def generate_spike_times(i):\n", "    gen = lambda: Sequence(numpy.add.accumulate(numpy.random.exponential(1000.0/input_rate, size=number)))\n", "    if hasattr(i, \"__len__\"):\n", "        return [gen() for j in i]\n", "    else:\n", "        return gen()\n", "#assert generate_spike_times(0).max() > simtime\n", "\n", "spike_source = sim.Population(n, sim.SpikeSourceArray(spike_times=generate_spike_times))\n", "\n", "\n", "input_conn1 = sim.Projection(spike_source, coba, sim.FixedProbabilityConnector(.1), sim.StaticSynapse(weight=w/4.,delay=syn_delay))\n", "input_conn2 = sim.Projection(spike_source, cuba, sim.FixedProbabilityConnector(.1), sim.StaticSynapse(weight=w*10,delay=syn_delay))\n", "\n", "#-------- Recording -------------\n", "spike_source.record('spikes')\n", "\n", "coba.record('spikes')\n", "coba[0:n].record(('v', 'gsyn_exc'))\n", "\n", "cuba.record('spikes')\n", "cuba[0:n].record(('v'))\n", "\n", "# === Run simulation ===========================================================\n", "\n", "sim.run(simtime)\n", "\n", "print(\"COBA Mean firing rate: \", coba.mean_spike_count()*1000.0/simtime, \"Hz\")\n", "print(\"CUBA Mean firing rate: \", cuba.mean_spike_count()*1000.0/simtime, \"Hz\")\n", "\n", "# === Clean up and quit ========================================================\n", "\n", "sim.end()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["###  Le processus de Poisson\n", "\n", "Afin de simuler une entr\u00e9e bruit\u00e9e nous utilisons un processus de Poisson. Le processus de Poisson est un processus stochastique, qui permet la g\u00e9n\u00e9ration d'une s\u00e9quence de potentiels d'action, au cours d'un certain laps de temps et, en fonction d'une probabilit\u00e9 d'occurence donn\u00e9e. Il est dit homog\u00e8ne dans le cas o\u00f9 la probabilit\u00e9 d'occurence d'un potentiel d'action pour tout temps $t$ est la m\u00eame, et o\u00f9 elle ne d\u00e9pend donc pas des potentiels d'action survenus \u00e0 $t-n\\Delta t$,  $n$ \u00e9tant un naturel non nul et $\\Delta t$, le pas de temps <cite data-cite=\"Burkitt_2006\">(Burkitt, 2006)</cite>. Une telle entr\u00e9e nous servira ainsi de source d'activit\u00e9 pour les r\u00e9seaux que nous impl\u00e9menterons.\n", "\n", "###  Effet de variations de param\u00e8tres cellulaires et non-cellulaires\n", "\n", "Ici, nous impl\u00e9mentons un r\u00e9seau compos\u00e9 de deux populations, une population A dite source et une population B. L'activit\u00e9 de la population A est d\u00e9finie par un processus de poisson homog\u00e8ne et la population B comporte des neurones COBA. Les deux populations sont compos\u00e9es chacune de deux neurones et sont connect\u00e9es entre elles par une projection de type \"all to all\". C'est \u00e0 dire que chacun des neurones de la population A est connect\u00e9 \u00e0 tous les neurones de la population B. \n", "Une premi\u00e8re solution est de r\u00e9soudre analytiquement les \u00e9quation diff\u00e9rentielles  <cite data-cite=\"Burkitt_2006a\">(Burkitt, 2006)</cite>. Une exploration num\u00e9rique de diff\u00e9rents param\u00e8tres est ici privil\u00e9gi\u00e9e. Cette \u00e9tude permet de tester l'effet de ces param\u00e8tres sur le taux de d\u00e9charge mesur\u00e9 en sortie du r\u00e9seau, c'est \u00e0 dire l'activit\u00e9 de la population B. En effet, la variation de ces param\u00e8tres sur un petit r\u00e9seau de neurones permet d'observer rapidement les ph\u00e9nom\u00e8nes qu'elle provoque. Les param\u00e8tres par d\u00e9faut sont inspir\u00e9s de mod\u00e8les canoniques (http://neuralensemble.org/trac/PyNN/wiki/StandardModels):\n", "\n", "Pour chaque param\u00e8tre \u00e9tudi\u00e9, plusieurs simulations du mod\u00e8le sont lanc\u00e9es avec diff\u00e9rentes valeurs du param\u00e8tre. Et pour chaque simulation, le taux de d\u00e9charge neuronal moyen de la population B est r\u00e9cup\u00e9r\u00e9. Les r\u00e9sultats sont alors affich\u00e9s dans une courbe de taux de d\u00e9charge en fonction d'une variation d'un param\u00e8tre. Les r\u00e9sultats de ces simulations peuvent \u00eatre interp\u00e9t\u00e9s \u00e0 partir des courbes de r\u00e9ponse entr\u00e9e-sortie (I-F) sur les figures et de fa\u00e7on qualitative concernant le taux de d\u00e9charge de la population B, nous observons sur une augmentation de la valeur des param\u00e8tres respectifs :\n", "\n", "- taux de d\u00e9charge de la source: une augmentation quasi-lin\u00e9aire du taux de d\u00e9charge,\n", "- poids de la source : une augmentation quasi-lin\u00e9aire \u00e9galement, \n", "- d\u00e9lai synaptique :  pas de variation,\n", "- tau_m : une augmentation logarithmique,\n", "- tau_syn_E : une augmentation lin\u00e9aire,\n", "- tau_syn_I : pas de variation,\n", "- e_rev_E : une augmentation par paliers,\n", "- e_rev_I : pas de variation,\n", "- v_rest : une augmentation quasi lin\u00e9aire,\n", "- v_thresh : une diminution en fonction puissance,\n", "- v_reset : une l\u00e9g\u00e8re augmentation,\n", "- tau_refrac : une l\u00e9g\u00e8re diminution,\n", "- cm : une diminution exponentielle\n", "\n", "Ces observations confirment bien la r\u00e9solution analytique des \u00e9quations diff\u00e9rentielles et permettent de contr\u00f4ler que l'on est dans un bon r\u00e9gime <cite data-cite=\"Burkitt_2006a\">(Burkitt, 2006)</cite>.\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": ["import pandas as ps\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "import pyNN.nest as sim\n", "#import pyNN.brian as sim\n", "\n", "from pyNN.parameters import Sequence\n", "from pyNN.random import RandomDistribution as rnd\n", "\n", "import os"], "outputs": [], "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "collapsed": false, "deletable": true}}, {"source": ["## Le r\u00e9seau de neurones al\u00e9atoire\n", "\n", "Afin de ne pas avoir \u00e0 faire face \u00e0 de nombreuses difficult\u00e9s \u00e0 la fois, il convient de d\u00e9velopper et d'\u00e9tudier le mod\u00e8le de d\u00e9tection d'orientation \u00e9tape par \u00e9tape. Ainsi, avant de traiter le r\u00e9seau en Ring, une g\u00e9n\u00e9ralisation de celui-ci, le r\u00e9seau al\u00e9atoire ou \"random neural network\" (RNN), est d'abord d\u00e9velopp\u00e9e et explor\u00e9e. Cette \u00e9tape permet, dans un premier temps, de n\u00e9gliger la topologie du r\u00e9seau pour se concentrer sur la connectique ainsi que sur la recherche et d\u00e9monstration d'un \u00e9tat d'\u00e9quilibre de celui-ci. \n", "\n", "Le RNN utilis\u00e9 est un r\u00e9seau comportant mille neurones propres au r\u00e9seau et cinq cents neurones pour la source, et est constitu\u00e9 de trois populations: une population source, une population excitatrice et une population inhibitrice. La population source mise \u00e0 part, le r\u00e9seau contient des neurones du m\u00eame type que ceux utilis\u00e9s dans le r\u00e9seau feedforward \u00e9tudi\u00e9 jusqu'\u00e0 pr\u00e9sent.\n", "\n", "D\u00e8s \u00e0 pr\u00e9sent, nous parlons de \"connexion\" pour d\u00e9signer une connexion synaptique entre neurones, et de \"projection\" lorsque nous faisons r\u00e9f\u00e9rence \u00e0 l'ensemble des connexions des neurones d'une population A aux neurones d'une autre population B. Le poids d'une projection, est alors le poids de toutes les connexions synaptiques de cette projection.\n", "Aussi le RRNN se d\u00e9cline sous deux formes :\n", "- une forme feed-forward dans laquelle seules les populations source et exitatrice sont connect\u00e9es.\n", "- une forme r\u00e9currente appel\u00e9e \"random recurrent neural network\" (RRNN)\n", "\n", "Dans ces deux formes du r\u00e9seau, la population source comporte des neurones qui d\u00e9chargent selon un processus de Poisson homog\u00e8ne. La population excitatrice, E, re\u00e7oit l'activit\u00e9 \u00e9mise par la source via une projection de type \"one to one\". C'est-\u00e0-dire que chaque neurone de la population source est connect\u00e9 \u00e0 un seul neurone de la population E. Dans un RRNN, l'activit\u00e9 transform\u00e9e par cette derni\u00e8re excite la population inhibitrice. Et, la population inhibitrice, I, dont les neurones poss\u00e8dent, hormis leur facult\u00e9 d'inhibition, les m\u00eames propri\u00e9t\u00e9s que les neurones excitateurs, inhibe E. En outre, E et I poss\u00e8dent des connexions r\u00e9currentes et de fa\u00e7on arbitraire (car on peut r\u00e9gler les poids d'interaction), les populations E et I poss\u00e8dent le m\u00eame nombre de neurones <cite data-cite=\"Brunel2000\">(Brunel, 2000)</cite>."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"source": ["## Le RNN feed-forward\n", "\n", "### Effet de variations de param\u00e8tres cellulaires\n", "\n", "Il est primordial de bien choisir les valeurs des param\u00e8tres cellulaires pour \u00e9viter que l'activit\u00e9 du r\u00e9seau soit trop basse ou trop \u00e9lev\u00e9e. Cette exp\u00e9rience contr\u00f4le est n\u00e9cessaire pour rester dans un r\u00e9gime d'activit\u00e9 o\u00f9 la manipulation des param\u00e8tres non cellulaires aura un effet observable sur l'activit\u00e9 du r\u00e9seau.\n", "\n", "Pour ce faire, le m\u00eame proc\u00e9d\u00e9 d'exploration utilis\u00e9 pour le r\u00e9seau feed-forward est effectu\u00e9 ici. Ainsi, l'effet de la variation de chacun des param\u00e8tres cellulaires sur le taux de d\u00e9charge peut \u00eatre observ\u00e9. Nous rappelons que pour chaque param\u00e8tre \u00e9tudi\u00e9, plusieurs simulations du mod\u00e8le sont lanc\u00e9s avec diff\u00e9rentes valeurs du param\u00e8tre. Et, pour chaque simulation, le taux de d\u00e9charge neuronal moyen de la population B est r\u00e9cup\u00e9r\u00e9. Les r\u00e9sultats sont alors affich\u00e9s dans une courbe du taux de d\u00e9charge mesur\u00e9 en fonction d'une variation d'un param\u00e8tre.\n", "\n", "Les r\u00e9sultats obtenus v\u00e9rifient bien le fait que les param\u00e8tres cellulaires sont bien choisis."], "cell_type": "markdown", "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}}, {"source": ["### Choix de la dynamique de la synapse (fonction de decay)\n", "\n", "Les mod\u00e8les de neurones ``IFcondexp`` et ``IFcondalpha`` sont ici \u00e0 nouveau compar\u00e9s, dans le cadre du RRNN cette fois. Le but est d'\u00e9viter d'avoir un r\u00e9seau qui sature trop facilement afin que les manipulations effectu\u00e9es aient un effet observable.\n", "Les param\u00e8tres de taux de d\u00e9charge d'entr\u00e9e et de poids de l'entr\u00e9e sont manipul\u00e9s. Comme cela a pu \u00eatre effectu\u00e9 pr\u00e9c\u00e9demment, pour chacun de ces param\u00e8tres, le taux de d\u00e9charge moyen des neurones du r\u00e9seau est r\u00e9cup\u00e9r\u00e9 et une courbe est g\u00e9n\u00e9r\u00e9e.\n", "\n", "Le rapport F/I est plus important pour un decay en alpha function. Afin d'\u00e9viter des saturations de l'activit\u00e9 du r\u00e9seau et compte tenu des r\u00e9sultats obtenus avec les deux types de neurones, je choisis de conserver l'IF cond exp."], "cell_type": "markdown", "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}}, {"execution_count": 2, "cell_type": "code", "source": ["import numpy as np\n", "from RRNN import RRNN\n", "\n", "net = RRNN(ring=False, recurrent=False)\n", "\n", "datapath_exp = '/tmp/OB-V1_data/cond_exp' + tag\n", "\n", "n_sim_each = 20\n", "\n", "sim_list = [\n", "            ('input_rate' , net.sim_params['input_rate']*np.logspace(-.1, .3, n_sim_each)),\n", "            ('w_input_exc', net.sim_params['w_input_exc']*np.logspace(-.3, .3, n_sim_each)),\n", "]\n", "\n", "net.paramRole(sim_list, datapath=datapath_exp, f_rate_max=None)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": ["net = RRNN(ring=False, recurrent=False)\n", "net.sim_params['neuron_model'] = 'cond_alpha'\n", "datapath_alpha = '/tmp/OB-V1_data/cond_alpha' + tag\n", "net.paramRole(sim_list, datapath=datapath_alpha, f_rate_max=None)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### R\u00f4le du taux de d\u00e9charge de l'entr\u00e9e\n", "\n", "L'activit\u00e9 neuronale de la population source est d\u00e9finie par un processus de Poisson homog\u00e8ne. \n", "L'activit\u00e9 de la population source est l'\u00e9nergie apport\u00e9e au RRNN. Il est alors important d'observer l'effet d'une manipulation de cette activit\u00e9 sur le comportement du r\u00e9seau.\n", "\n", "Pour chaque valeur de taux de d\u00e9charge de la population source une simulation du RRNN est ex\u00e9cut\u00e9e et des rasterplots des populations source, exciatrice (E) et inhibitrice (I) sont affich\u00e9s.\n", "\n", "Il est remarqu\u00e9 qu'une augmentation suffisante de l'activit\u00e9 de la population source induit une augmentation d'activit\u00e9 dans les populations E et I. Cette discontinuit\u00e9 sugg\u00e8re l'existence d'un filtre entre la population source et les deux autres.\n", "\n", "### R\u00f4le du poids de l'entr\u00e9e\n", "\n", "Les rasterplots g\u00e9n\u00e9r\u00e9s sur une manipulation de l'activit\u00e9 de la population source montrent l'existence de filtres entre l'activit\u00e9 de la source et l'activit\u00e9 du r\u00e9seau. Le poids de l'entr\u00e9e est un de ces filtres. Ce param\u00e8tre d\u00e9termine les valeurs des poids des connexions synaptiques entre les neurones de la source et les neurones de la population E. Ici, son effet sur le comportement du r\u00e9seau est \u00e9tudi\u00e9.\n", "\n", "Pour cela, un rasterplot des trois populations est g\u00e9n\u00e9r\u00e9 par valeur du poids de l'entr\u00e9e\n", "\n", "Il s'av\u00e8re que l'augmentation du poids de l'entr\u00e9e induit une augmentation de l'activit\u00e9 du r\u00e9seau.\n", "            \n", "### Courbes de taux de d\u00e9charge en fonction de l'activit\u00e9 et du poids de l'entr\u00e9e\n", "\n", "Les rasterplots permet d'avoir un aper\u00e7u qualitatif de l'activit\u00e9 d'une population neuronale. Cependant, il est n\u00e9cessaire de pouvoir observer de fa\u00e7on quantitative cette activit\u00e9. Ici, les effets de la manipulation de l'activit\u00e9 de la population source ainsi que du poids de celle-ci sur l'activit\u00e9 du r\u00e9seau sont r\u00e9examin\u00e9s. \n", "\n", "Pour chacun des deux param\u00e8tres, une simulation du mod\u00e8le est ex\u00e9cut\u00e9e par valeur prise par le param\u00e8tre \u00e9tudi\u00e9 et le taux de d\u00e9charge neuronal moyen des populations E et I est recup\u00e9r\u00e9. Chaque point d'une courbe \u00e9tant d\u00e9fini par le couple (valeur du param\u00e8tre, taux de d\u00e9charge), une courbe repr\u00e9sente donc la variation de taux de d\u00e9charge en fonction de la manipulation d'un param\u00e8tre.\n", "\n", "Il s'av\u00e8re que le taux de d\u00e9charge augmente bien quand l'activit\u00e9 de la population source ou le poids de l'entr\u00e9e augmentent.           \n"], "cell_type": "markdown", "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}}, {"execution_count": 2, "cell_type": "code", "source": ["from RRNN import RRNN\n", "import numpy as np\n", "\n", "net = RRNN(ring=False, recurrent=False)\n", "_ = net.variationRaster('input_rate', net.sim_params['input_rate'] * np.logspace(-1, 1, n_sim_each))\n", "#_ = net.variationRaster('input_rate', net.sim_params['input_rate'] * np.logspace(-.2, .2, 10))"], "outputs": [], "metadata": {"button": false, "new_sheet": false, "collapsed": false, "scrolled": true, "deletable": true, "run_control": {"read_only": false}}}, {"execution_count": 3, "cell_type": "code", "source": ["_ = net.variationRaster('w_input_exc', net.sim_params['w_input_exc']*np.logspace(-1, 1, n_sim_each))"], "outputs": [], "metadata": {"scrolled": true, "collapsed": false}}, {"source": ["### Effet d'une covariation de l'activit\u00e9 d'entr\u00e9e et de son poids :\n", "\n", "Il a \u00e9t\u00e9 observ\u00e9 pr\u00e9c\u00e9demment que, l'activit\u00e9 de la population source et les poids des connexions, entre la population source et la population E, sont positivement corr\u00e9l\u00e9s \u00e0 l'activit\u00e9 du r\u00e9seau. Aussi, ces deux param\u00e8tres paraissent d\u00e9pendants l'un de l'autre. Il est donc n\u00e9cessaire de v\u00e9rifier cette interd\u00e9pendance.\n", "\n", "Pour cela, des rasterplots sont g\u00e9n\u00e9r\u00e9s \u00e0 partir d'une covariation de ces deux param\u00e8tres. Cette covariation est effectu\u00e9e en pond\u00e9rant chacun des deux param\u00e8tres avec un coefficient distinct. Ces deux coefficients vont parcourir le m\u00eame intervalle de valeurs, mais l'un dans un sens contraire \u00e0 l'autre. De telle sorte, en fait, que le produit des deux param\u00e8tres soit toujours le m\u00eame.\n", "\n", "Nous observons que seule l'activit\u00e9 de la population source varie. L'activit\u00e9 observ\u00e9e dans les populations E et I n'\u00e9volue pas, car le flux d'entr\u00e9e, d\u00e9fini par l'activit\u00e9 de la population source et son poids, est en fait toujours le m\u00eame. Nous \u00e9tudions maintenant les propri\u00e9t\u00e9s de la connectique interne au r\u00e9seau."], "cell_type": "markdown", "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}}, {"execution_count": 4, "cell_type": "code", "source": ["from RRNN import RRNN\n", "import numpy as np\n", "\n", "net = RRNN(ring=False, recurrent=False)\n", "rateI = net.sim_params['input_rate']\n", "wIe = net.sim_params['w_input_exc']\n", "\n", "_ = net.doubleVariationRaster_P2P('input_rate', rateI*np.logspace(-1, 1, 10), \n", "                                  'w_input_exc', wIe*np.logspace(1, -1, 10))"], "outputs": [], "metadata": {"scrolled": true, "button": false, "new_sheet": false, "run_control": {"read_only": false}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"source": ["## Le RRNN\n", "\n", "###  Le r\u00e9seau r\u00e9current al\u00e9atoire: R\u00f4le du poids global\n", "\n", "Maintenant que le flux d'entr\u00e9e est d\u00e9fini, il s'agit ici d'\u00e9tudier les effets de la manipulation du poids global $W$. Modifier ce param\u00e8tre revient \u00e0 changer tous les poids, hormis celui de la projection entre la source et la population E.\n", "\n", "Une simulation de 100 ms est ex\u00e9cut\u00e9e pour chaque valeur de $W$ appartenant \u00e0 l'intervalle de sa variation. Des rasterplots repr\u00e9sentant l'activit\u00e9 des trois populations du mod\u00e8le sont alors g\u00e9n\u00e9r\u00e9s.\n", "\n", "Une augmentation de $W$ modifie bien le comportement des populations excitatrice et inhibitrice. Nous pouvons observer pour certaines valeurs de $W$, dans les populations E et I, un comportement de type Asynchronous Regular qui est un des quatres \u00e9tats d'activit\u00e9 d\u00e9crits par Brunel, dont nous parlerons plus tard."], "cell_type": "markdown", "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}}, {"execution_count": 6, "cell_type": "code", "source": ["from RRNN import RRNN\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "n_sim_each = 20\n", "net = RRNN(ring=False, recurrent=True)\n", "w_0 = net.w\n", "\n", "for zoom in [1., .3]:\n", "    print('ZOOM ', zoom)\n", "    for net.w in w_0 * np.logspace(-zoom, zoom, n_sim_each):\n", "        net.init_params()\n", "        #print(net.w, net.sim_params['w_exc_exc'])\n", "        df, spikesE, spikesI = net.model()\n", "        net.Raster(df, spikesE, spikesI, input=True, title='weight = {}'.format(net.w))\n", "        plt.show()"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": ["### Courbe de taux de d\u00e9charge en fonction des poids synaptiques\n", "\n", "Une fois l'effet d'une variation du poids global $W$ observ\u00e9, des tests plus sp\u00e9cifiques doivent \u00eatre effectu\u00e9s. Hormis les connexions de la source au r\u00e9seau, nous rappelons que le mod\u00e8le comporte quatre projections. Soient E et I, la population excitatrice et inhibitrice l'ensemble des poids entre ces deux populations est : {$W_{EI}$, $W_{EE}$, $W_{IE}$, $W_{II}$}. L'effet des poids de chacune des quatre projections sur le taux de d\u00e9charge des neurones du r\u00e9seau est \u00e9tudi\u00e9. \n", "\n", "Pour chaque type de connexion, plusieurs simulations du mod\u00e8le sont lanc\u00e9s avec diff\u00e9rentes valeurs du poids synaptique des connexions de ce type. Et pour chaque simulation, le taux de d\u00e9charge neuronal moyen des populations E et I est r\u00e9cup\u00e9r\u00e9. Les r\u00e9sultats sont alors affich\u00e9s dans une courbe de taux de d\u00e9charge en fonction d'une variation de poids synaptique.\n", "\n", "Nous remarquons que l'augmentation de $W_{EI}$ comme $W_{IE}$, le poids des connexions lat\u00e9rales, induit une diminution du taux de d\u00e9charge en sortie du r\u00e9seau. Aussi, l'augmentation de $W_{EE}$ et $W_{II}$, le poids des connexions r\u00e9currentes, provoque l'effet inverse."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": ["import numpy as np\n", "from RRNN import RRNN\n", "n_sim_each = 20\n", "net = RRNN(ring=False, recurrent=True)"], "outputs": [], "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "collapsed": false, "deletable": true}}, {"source": ["### Courbes de taux de d\u00e9charge en fonction des param\u00e8tres de sparseness\n", "\n", "Les effets observ\u00e9s de la manipulation des poids des diff\u00e9rentes projections g\u00e9n\u00e8rent des variations de flux d'activit\u00e9 entre les diff\u00e9rentes populations du r\u00e9seau. Mais ces flux ne d\u00e9pendent pas seulement des poids synaptiques. Ils d\u00e9pendent \u00e9galement du nombre de connexions existantes entre les populations.\n", "\n", "Le simulateur utilis\u00e9 permet de caract\u00e9riser, de diff\u00e9rentes mani\u00e8res, la fa\u00e7on de connecter les neurones de deux populations. Parmi les options propos\u00e9es, se trouve un connecteur \u00e0 probabilit\u00e9 fixe qui va connecter deux populations selon un param\u00e8tre de \"sparseness\". La \"sparseness\" est un param\u00e8tre d\u00e9finissant une probabilit\u00e9 de connexion synaptique entre les neurones de deux populations. Si la sparseness d'une projection est \u00e9gale \u00e0 1, la projection est de type \"all to all\". Chaque projection poss\u00e8de donc un param\u00e8tre de \"sparseness\".\n", "\n", "Ici, l'\u00e9volution du taux de d\u00e9charge en fonction d'une variation de \"sparseness\" des diff\u00e9rentes projections est \u00e9tudi\u00e9e. Ces analyses permettent d'appr\u00e9cier la contribution de ces diff\u00e9rents param\u00e8tres \u00e0 l'activit\u00e9.\n", "\n", "Comme cela \u00e0 pu \u00eatre fait pour d'autres param\u00e8tres, une courbe de taux de d\u00e9charge en fonction d'une variation de sparseness est g\u00e9n\u00e9r\u00e9e pour chaque projection.\n", "\n", "L'augmentation de la probabilit\u00e9 de connexion pour les projections EI et IE (lat\u00e9rales) provoque une diminution du taux de d\u00e9charge. Alors que la m\u00eame manipulation pour les projections EE et II (recurrentes) provoque une augmentation du taux de d\u00e9charge. \n", "\n", "Les r\u00e9sultats sont similaires \u00e0 ceux obtenus avec la variation des poids synaptiques, ce qui am\u00e8ne \u00e0 consid\u00e9rer la modification de la \"sparseness\" ou des poids d'une m\u00eame projection, comme une mani\u00e8re ou une autre de manipuler le flux d'activit\u00e9 entre les populations que cette projection relie.\n", "C'est pourquoi dans la suite de ce projet, une manipulation du flux d'activit\u00e9 d'une population A \u00e0 une autre population B sera impl\u00e9ment\u00e9e simplement par une modification du poids de la projection AB.\n"], "cell_type": "markdown", "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}}}, {"execution_count": 2, "cell_type": "code", "source": ["import numpy as np\n", "from RRNN import RRNN\n", "n_sim_each = 20\n", "net = RRNN(ring=False, recurrent=True)\n", "\n", "sim_params = net.sim_params\n", "cell_params = net.cell_params\n", "sim_list = [\n", "            #('p' , sim_params['p'] * np.logspace(-.2, .2, n_sim_each)),\n", "            ('c_exc_inh' , sim_params['c_exc_inh']*np.logspace(-1, 1, n_sim_each)),\n", "            ('c_inh_exc' , sim_params['c_inh_exc']*np.logspace(-1, 1, n_sim_each)),\n", "            ('c_exc_exc' , sim_params['c_exc_exc']*np.logspace(-1, 1, n_sim_each)),\n", "            ('c_inh_inh' , sim_params['c_inh_inh']*np.logspace(-1, 1, n_sim_each))\n", "]\n", "\n", "net.paramRole(sim_list, f_rate_max=None, datapath='/tmp/RRNN_topo1' + tag)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## Les \u00e9tats du r\u00e9seau\n", "\n", "Brunel d\u00e9crit quatre \u00e9tats du RRNN :\n", "\n", "- Synchronous Regular : Les activations neuronales sont synchrones et les intervalles inter-d\u00e9charges pour chaque neurone sont identiques\n", "- Synchronous Irregular : Les activations neuronales sont synchrones et les intervalles inter-d\u00e9charges sont diff\u00e9rents\n", "- Asynchronous Regular : Les neurones ne s'activent pas en m\u00eame temps et les intervalles inter-d\u00e9charges sont identiques\n", "- Asynchronous Irregular : Les neurones ne s'activent pas en m\u00eame temps et les intervalles inter-d\u00e9charges sont diff\u00e9rents\n", "\n", "Il fait \u00e9galement r\u00e9f\u00e9rence \u00e0 un param\u00e8tre de coupling $g$, qui r\u00e9gulerait le poids de l'inhibition par rapport \u00e0 celui de l'excitation. La modification d'un tel param\u00e8tre permettrait une transition entre les diff\u00e9rents \u00e9tats du r\u00e9seau <cite data-cite=\"Brunel2000\">(Brunel, 2000)</cite>.\n", "\n", "Nous faisons alors des suppositions sur la d\u00e9finition de $g$, et nous observons l'effet de $g$ sur le comportement du RRNN, pour chacune de ces suppositions.\n", "\n", "### D\u00e9finir le coupling\n", "\n", "La r\u00e9elle d\u00e9finition du param\u00e8tre de coupling n'est pas \u00e9vidente \u00e0 appr\u00e9hender. La seule certitude que nous avons est que le param\u00e8tre de coupling doit \u00eatre un ratio de certains poids par rapport au poids global $W$. C'est pourquoi diff\u00e9rents ratio de poids synaptiques pouvant correspondre \u00e0 $g$ sont ici test\u00e9s, \u00e0 savoir :\n", "\n", "- $g = W_{IE}, W_{II} / W$\n", "- $g = W_{IE}, W_{EI} / W$\n", "- $g = W_{EI}, W_{II} / W$\n", "\n", "Chacune des trois d\u00e9finitions possibles de $g$ correspond \u00e0 des variations de poids de certaines projections par rapport \u00e0 d'autres. Aussi, il a pr\u00e9c\u00e9demment \u00e9t\u00e9 fait mention d'une m\u00e9thode permettant d'obtenir une courbe de taux de d\u00e9charge en fonction de la variation de param\u00e8tre.\n", "Une m\u00e9thode similaire est ici utilis\u00e9e, mais cette fois, les taux de d\u00e9charge sont calcul\u00e9s \u00e0 partir d'une covariation de deux param\u00e8tres de poids synaptique et ce, pour chacune des d\u00e9finitions de $g$.\n", "\n", "Il semble que si $g$ est d\u00e9fini par le ratio du poids des connexions lat\u00e9rales par rapport \u00e0 $W$,\n", "l'activit\u00e9 du r\u00e9seau diminue quand $g$ cro\u00eet. Ce qui n'est pas sans rappeler les observations effectu\u00e9es du taux de d\u00e9charge lors de la manipulation des poids des diff\u00e9rentes projections internes au r\u00e9seau.\n", "\n", "### R\u00f4le du coupling\n", "\n", "Ainsi, faire varier $g$ devrait provoquer une transition de phase des \u00e9tats d'activit\u00e9 du r\u00e9seau. Pour observer un tel comportement, la mesure du taux de d\u00e9charge neuronal, bien qu'elle aie l'avantage d'amener \u00e0 des observations quantitatives, n'est pas suffisamment informative. C'est pourquoi une m\u00e9thode qualitative, mais n\u00e9ammoins plus pertinente dans ce contexte, comme la g\u00e9n\u00e9ration de rasterplot, est utilis\u00e9e.\n", "\n", "Une simulation du RRNN est ex\u00e9cut\u00e9e pour chaque valeur prise par $g$. Les spikes des neurones des populations source, excitatrice et inhibitrice sont r\u00e9cup\u00e9r\u00e9s et affich\u00e9s dans le rasterplot. Ainsi, il est possible de savoir quels neurones ont d\u00e9charg\u00e9s et quand.  \n", "\n", "Un changement brusque du comportement du r\u00e9seau est observ\u00e9 lorsque $g$ d\u00e9passe une certaine valeur. Par la suite, l'\u00e9volution de l'\u00e9tat du r\u00e9seau se fait de mani\u00e8re continue."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "import pandas as ps\n", "from RRNN import RRNN\n", "\n", "n_sim_each = 20\n", "net = RRNN(ring=False, recurrent=True)\n", "\n", "net.setParams(['w_exc_inh', 'w_exc_exc', 'w_inh_exc', 'w_inh_inh'],\n", "              [net.w, net.w, net.w, net.w])"], "outputs": [], "metadata": {"button": false, "new_sheet": false, "run_control": {"read_only": false}, "collapsed": false, "deletable": true}}, {"source": ["### L'\u00e9tat \u00e9quilibr\u00e9\n", "\n", "Nous avons pr\u00e9c\u00e9demment \u00e9voqu\u00e9 les diff\u00e9rents \u00e9tats de l'activit\u00e9 du RRNN.\n", "L'\u00e9tat \u00e9quilibr\u00e9 que nous recherchons correspond \u00e0 l'\u00e9tat Asynchronous Irregular. Il correspond \u00e0 un \u00e9quilibre entre l'excitation et l'inhibition. Un tel \u00e9quilibre a fait l'objet d'\u00e9tudes th\u00e9oriques qui montrent qu'un RRNN balanc\u00e9 produit une activit\u00e9 bruit\u00e9e, proche de celle pouvant \u00eatre g\u00e9n\u00e9r\u00e9e par un processus de Poisson. Aussi, la r\u00e9ponse d'un RRNN \u00e9quilibr\u00e9 \u00e0 une entr\u00e9e est lin\u00e9aire et plus rapide que l'int\u00e9gration d'information pouvant \u00eatre effectu\u00e9e par un seul neurone <cite data-cite=\"Brunel2000\">(Brunel, 2000)</cite>. Ces propri\u00e9t\u00e9s sont int\u00e9ressantes pour la mod\u00e9lisation de r\u00e9seaux de neurones corticaux, tels que le cortex visuel primaire. En effet, nous rappelons que la s\u00e9lectivit\u00e9 \u00e0 l'orientation au sein des pinwheels et chez le rat est possible si le r\u00e9seau est dans cet \u00e9tat. \n", "\n", "Maintenant que des outils ont \u00e9t\u00e9 d\u00e9velopp\u00e9s pour observer des variations de comportement du RRNN, la recherche d'\u00e9tats balanc\u00e9s du r\u00e9seau peut d\u00e9buter. Nous introduisons \u00e0 pr\u00e9sent des indices utiles pour caract\u00e9riser des aspects de l'activit\u00e9 :\n", "\n", "- $\\frac{dF}{dI}$ est la d\u00e9riv\u00e9e du taux de d\u00e9charge en sortie par rapport au taux de d\u00e9charge de la population source. Cet indice correspond au gain du r\u00e9seau produit par les propri\u00e9t\u00e9s cellulaires et les propri\u00e9t\u00e9s des connexions entre les populations.\n", "- $CV$ est le coefficient de variation de l'intervalle inter-d\u00e9charge. Il est d\u00e9fini par le ratio de l'\u00e9cart-type de la distribution des intervalles sur leur moyenne. C'est une mesure de la variabilit\u00e9 des intervalles inter-d\u00e9charges.\n", "\n", "$$\n", "CV = \\frac{\\sqrt{ \\langle T^2 \\rangle - \\langle T \\rangle ^2} }{\\langle T \\rangle}\n", "$$\n", "\n", "Nous impl\u00e9mentons \u00e9galement une m\u00e9thode d'optimisation permettant d'obtenir des param\u00e8tres de poids et de coupling satisfaisant certaines contraintes. Les indices \u00e9voqu\u00e9s plus haut sont autant de contraintes devant \u00eatre satisfaites."], "cell_type": "markdown", "metadata": {}}, {"source": ["### Optimisation du poids global\n", "\n", "Ici, nous tentons de trouver la valeur de coupling qui satisfait des contraintes de CV et de $\\frac{dF}{dI}$ traduisant un \u00e9tat AI (i.e. l'\u00e9tat balanc\u00e9) du r\u00e9seau.\n", "\n", "L'algorithme d'optimisation est ex\u00e9cut\u00e9 sur une variation de coupling. Une figure qui nous permet d'estimer la valeur de $g$ maximisant le CV et $\\frac{dF}{dI}$ est alors g\u00e9n\u00e9r\u00e9e.\n", "Par la suite, la valeur de g obtenue serait ensuite fournie au mod\u00e8le afin de pouvoir \u00e9valuer l'excitabilit\u00e9 du RRNN balanc\u00e9."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": ["from RRNN import RRNN\n", "import numpy as np\n", "\n", "net = RRNN(ring=False, recurrent=True)\n", "df = net.multiOptimisation(np.logspace(-1, 1, 30)*net.w, 'w', \n", "                           datapath='/tmp/OB-V1_data/RRNN_optw_' + tag)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"source": ["### Optimisation du coupling $g$\n", "\n", "Ici, nous tentons de trouver la valeur de coupling qui satisfait des contraintes de CV et de $\\frac{dF}{dI}$ traduisant un \u00e9tat AI (i.e. l'\u00e9tat balanc\u00e9) du r\u00e9seau.\n", "\n", "L'algorithme d'optimisation est ex\u00e9cut\u00e9 sur une variation de coupling. Une figure qui nous permet d'estimer la valeur de $g$ maximisant le CV et $\\frac{dF}{dI}$ est alors g\u00e9n\u00e9r\u00e9e.\n", "Par la suite, la valeur de g obtenue serait ensuite fournie au mod\u00e8le afin de pouvoir \u00e9valuer l'excitabilit\u00e9 du RRNN balanc\u00e9."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": ["from RRNN import RRNN\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import pandas as ps\n", "datapath = '/tmp/OB-V1_data/RRNN_optg_' + tag\n", "#datapath = '/tmp/OB-V1_data/RRNN_optg_tmp1469201041'"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": ["net = RRNN(ring=False, recurrent=True)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": ["df = net.multiOptimisation(np.logspace(-1, 1, 30)*net.g, 'g', datapath=datapath)"], "outputs": [], "metadata": {"scrolled": true, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"source": ["# Le Ring\n", "\n", "Une fois le RRNN cr\u00e9\u00e9, param\u00e9tr\u00e9 et optimis\u00e9, nous lui ajoutons certaines propri\u00e9t\u00e9s dans le but de le transformer en \"ring\". Ce dernier va nous permettre d'impl\u00e9menter le mod\u00e8le de la s\u00e9lectivit\u00e9 \u00e0 l'orientation reproduisant ce qui peut \u00eatre observ\u00e9 au sein des colonnes corticales du cortex visuel primaire.\n", "\n", "Le ring est un r\u00e9seau r\u00e9current disposant d'une certaine topologie. En effet, selon sa position dans le r\u00e9seau, un neurone poss\u00e8de une certaine s\u00e9lectivit\u00e9 \u00e0 l'orientation et les connexions sont locales dans l'espace des orientations. \n", "Quelques propri\u00e9t\u00e9s de la r\u00e9ponse \u00e0 l'orientation vont conditionner cette s\u00e9lectivit\u00e9 et induire un certain comportement du r\u00e9seau, en r\u00e9ponse \u00e0 une orientation pr\u00e9sent\u00e9e sur son entr\u00e9e :\n", "\n", "-  $m$ est l'angle d'orientation pr\u00e9fer\u00e9e d'un neurone. Cela signifie que ce dernier aura une r\u00e9ponse maximale si une orientation d'un angle $\\theta$, tel que $\\theta = m$, est pr\u00e9sent\u00e9e. Notons que le ring est construit de telle sorte que toutes les orientations sont cod\u00e9es avec une pr\u00e9cision de vingt minutes d'arc et qu'il est non orient\u00e9, ainsi $0\\leq m \\leq \\pi$.\n", "- la bandwidth $\\sigma$ est la largeur \u00e0 mi-hauteur de la courbe d'accord d'un neurone. Elle sert \u00e0 repr\u00e9senter la s\u00e9lectivit\u00e9 de la r\u00e9ponse neuronale \u00e0 d'autres orientations que celle pr\u00e9fer\u00e9e. Dans ce mod\u00e8le, les bandwidth ne sont pas param\u00e9tr\u00e9s par neurone mais plut\u00f4t par type de connexion entre les populations E et I. Nous impl\u00e9mentons \u00e9galement une bandwidth dans les connexions entre la source et la population E. Ainsi, nous cherchons \u00e0 ce que les neurones d'une colonne corticale aient une certaine bandwith de s\u00e9lectivit\u00e9 \u00e0 l'orientation du fait de leurs connexions avec d'autres colonnes.\n", "\n", "Une fonction d'accord est aussi impl\u00e9ment\u00e9e. Cette fonction permet de calculer le poids synaptique de chacune des connexions d'une projection \u00e0 partir des propri\u00e9t\u00e9s d\u00e9crites plus haut. De part sa g\u00e9n\u00e9ralit\u00e9, nous utiliserons une loi de Von Mises (loi normale circulaire) d\u00e9finie par :\n", "\n", "$$\n", "f(\\theta) = \\frac{1}{Z(\\kappa)} \\cdot e^{\\kappa{cos(2(\\theta - m))}}\n", "$$\n", "o\u00f9 $Z$ est la fonction de normalisation. Par analogie avec la d\u00e9viation standard d'une loi Gaussienne, on d\u00e9finit $\\kappa = \\frac {1}{\\sigma^{2}}$. Notons que $f(\\theta+\\pi) = f(\\theta)$.\n", "\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": ["import numpy as np\n", "import MotionClouds as mc\n", "import matplotlib.pyplot as plt\n", "downscale = 1\n", "fx, fy, ft = mc.get_grids(mc.N_X/downscale, mc.N_Y/downscale, 16)\n", "\n", "\n", "N_theta = 6\n", "bw_values = np.pi*np.logspace(-2, -5, N_theta, base=2)\n", "fig_width = 21\n", "\n", "\n", "fig, axs = plt.subplots(1, N_theta, figsize=(fig_width, fig_width/N_theta))\n", "for i_ax, B_theta in enumerate(bw_values):\n", "    mc_i = mc.envelope_gabor(fx, fy, ft, V_X=0., V_Y=0.,  \n", "                                         theta=np.pi/2, B_theta=B_theta)\n", "    im = mc.random_cloud(mc_i)\n", "                \n", "    axs[i_ax].imshow(im[:, :, 0], cmap=plt.gray())\n", "    axs[i_ax].text(5, 29, r'$B_\\theta=%.1f$\u00b0' % (B_theta*180/np.pi), color='white', fontsize=32)\n", "    axs[i_ax].set_xticks([])\n", "    axs[i_ax].set_yticks([])\n", "plt.tight_layout()\n", "fig.subplots_adjust(hspace = .0, wspace = .0, left=0.0, bottom=0., right=1., top=1.)\n", "\n", "import os\n", "fig.savefig(os.path.join('../figs', 'orientation_tuning.png'))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## Le ring non accord\u00e9\n", "\n", "### Effet de la bandwidth d'entr\u00e9e dans un r\u00e9seau feedforward\n", "\n", "Nous testons l'effet du changement de la bandwidth du signal d'entr\u00e9e sur le comportement du r\u00e9seau en d\u00e9sactivant toutes les projections sauf la projection d'entr\u00e9e.\n", "\n", "Pour cela, nous param\u00e9trons l'activit\u00e9 des neurones de la population source de telle sorte que celle-ci repr\u00e9sente une orientation de contraste de 90\u00b0. Une simulation du mod\u00e8le est ex\u00e9cut\u00e9e pour chaque valeur de bandwidth. Seule la projection de la source \u00e0 la population E est active, $W$ est nul. Des rasterplots des trois populations sont alors affich\u00e9s.\n", "\n", "Nous observons bien que certains neurones de la population E, ceux qui ont une r\u00e9ponse s\u00e9lective \u00e0 une orientation de ou proche de 90\u00b0 sont actifs. Aussi, les neurones de la population I sont inactifs."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": ["from RRNN import RRNN\n", "import numpy as np\n", "\n", "net = RRNN(ring=True, recurrent=False)\n", "_ = net.variationRaster('b_input', np.linspace(10, 180, 10))"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": ["### Effet de la bandwidth d'entr\u00e9e dans un ring recurrent dont les poids sont homog\u00e8nes\n", "\n", "Nous avons \u00e9tudi\u00e9 l'effet de la bandwidth du signal pr\u00e9sent\u00e9 en entr\u00e9e dans le cas o\u00f9 les connexions du r\u00e9seau, hormis les connexions entre la population source et la population E, sont d\u00e9sactiv\u00e9es. A pr\u00e9sent, nous \u00e9tudions son effet lorsque les poids synaptiques des connexions du ring sont homog\u00e8nes.\n", "\n", "Pour cela, nous param\u00e9trons les connexions des quatres diff\u00e9rentes projections de sorte qu'elles aient le m\u00eame poids synaptique. Puis, une simulation du mod\u00e8le est ex\u00e9cut\u00e9e pour chaque valeur de bandwidth, et les rasterplots des trois populations sont alors g\u00e9n\u00e9r\u00e9s.\n", "\n", "Nous observons que le poids $W$ a un effet sur l'activit\u00e9 ou non de la population inhibitrice, mais n'a aucun effet sur le nombre de neurones actifs dans la population excitatrice."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": ["net = RRNN(ring=False, recurrent=True)\n", "for w in np.logspace(-.5, 0.25, 5):\n", "    print('-------- w = {} --------'.format(w))\n", "    net.w = w\n", "    net.init_params()\n", "    _ = net.variationRaster('b_input', 180*np.logspace(0, -4, 5, base=2))"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": ["net = RRNN(ring=False, recurrent=True)\n", "for w_inh in np.logspace(-.5, 0.25, 5):\n", "    print('-------- w_inh = {} --------'.format(w_inh))\n", "    net.w_inh = w_inh\n", "    net.init_params()\n", "    _ = net.variationRaster('b_input', 180*np.logspace(0, -4, 5, base=2))"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": ["### Effet des bandwidths des projections internes\n", "\n", "Apr\u00e8s avoir \u00e9tudi\u00e9 l'effet de la bandwidth de l'entr\u00e9e sur un ring o\u00f9 les poids de toutes les connexions, sauf celles qui vont de la population source \u00e0 la population excitatrice, sont identiques, nous cherchons maintenant \u00e0 observer les effets d'une variation de bandwidth de chacune des quatre projections sur l'activit\u00e9 neuronale.\n", "\n", "Pour cela, nous excitons le r\u00e9seau avec une entr\u00e9e uniforme et nous g\u00e9n\u00e9rons des rasterplots pour chaque valeur de bandwidth d'une projection. Cette op\u00e9ration est r\u00e9p\u00e9t\u00e9e pour chacune des quatre projections.\n", "\n", "Nous constatons que, concernant l'activit\u00e9 des neurones de la population inhibitrice, seule une modification de la bandwidth pour la projection allant de la population excitatrice \u00e0 la population inhibitrice (EI) a un effet sur le nombre de neurones actifs."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": ["net = RRNN(ring=True, recurrent=True)\n", "net.sim_params['b_input'] = np.inf\n", "bw_values = 180*np.logspace(0, -4, 5, base=2)\n", "net.variationRaster('b_exc_exc', bw_values)"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": ["net.variationRaster('b_inh_exc', bw_values)"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": ["net.variationRaster('b_inh_inh', bw_values)"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": ["net.variationRaster('b_exc_inh', bw_values)"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": ["## Le ring accord\u00e9\n", "\n", "### Effet de la bandwidth d'entr\u00e9e\n", "\n", "Ici, nous \u00e9tudions l'effet d'une variation de la bandwidth d'entr\u00e9e sur l'activit\u00e9 neuronale au sein d'un ring, en observant la repr\u00e9sentation de l'entr\u00e9e que l'on lui soumet par diff\u00e9rentes populations.\n", "\n", "Nous param\u00e9trons le ring de telle sorte qu'il soit accord\u00e9. Puis, nous ex\u00e9cutons une simulation pour chaque valeur de bandwidth d'entr\u00e9e d\u00e9sir\u00e9e. Nous g\u00e9n\u00e9rons alors, pour chaque simulation, les rasterplots des populations source, excitatrice et inhibitrice.\n", "\n", "Nous constatons alors que l'activit\u00e9 des neurones de la population excitatrice reproduit l'activit\u00e9 de la population source. Seuls les neurones dont les orientations pr\u00e9f\u00e9r\u00e9es sont opr\u00e9sentes dans la distribution d'entr\u00e9e orientations sont actifs."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": ["net = RRNN(ring=True, recurrent=True)\n", "bws = 180*np.logspace(0, -4, 9, base=2)\n", "\n", "for bw in bws:\n", "    fig, ax = plt.subplots(figsize=(8,8))\n", "    net.setParams(['b_input'], [bw])\n", "    df, spikesE, spikesI = net.model()\n", "    _ = net.Raster(df, spikesE, spikesI,\n", "                  title='---------b_input = {} ---------'.format(str(bw)))\n", "\n", "    plt.show()"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": ["### Effet du poids global $W$\n", "\n", "Ici, nous \u00e9tudions l'effet du poids global $W$ sur la s\u00e9lectivit\u00e9 \u00e0 l'orientation du ring\n", "\n", "Pour cela, nous simulons le r\u00e9seau pour chaque valeur de $W$ et nous affichons les rasterplots repr\u00e9sentant l'activit\u00e9 des neurones des trois populations du mod\u00e8le."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": ["net = RRNN(ring=True, recurrent=True)\n", "ws = net.w * np.logspace(-.5, .3, 9)\n", "for w in ws:\n", "    fig, ax = plt.subplots(figsize=(8,8))\n", "    net.w = w\n", "    net.init_params()\n", "    df, spikesE, spikesI = net.model()\n", "    _ = net.Raster(df, spikesE, spikesI,\n", "                   title='--------- weight = {} ---------'.format(str(w)))\n", "\n", "    plt.show()"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": ["net = RRNN(ring=True, recurrent=True)\n", "ws = net.w_inh * np.logspace(-1, 1, 9)\n", "for w_inh in ws:\n", "    fig, ax = plt.subplots(figsize=(8,8))\n", "    net.w_inh = w_inh\n", "    net.init_params()\n", "    df, spikesE, spikesI = net.model()\n", "    _ = net.Raster(df, spikesE, spikesI,\n", "                   title='--------- weight_inh = {} ---------'.format(str(w_inh)))\n", "\n", "    plt.show()"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}, {"source": ["### Effet du coupling $g$\n", "\n", "Ici, nous \u00e9tudions l'effet d'une variation du coupling $g$ sur l'activit\u00e9 neuronale au sein d'un ring, pour observer d'\u00e9ventuels effets sur sa s\u00e9lectivit\u00e9 \u00e0 l'orientation.\n", "\n", "D'une mani\u00e8re analogue \u00e0 l'exp\u00e9rience pr\u00e9c\u00e9dente, nous ex\u00e9cutons des simulations pour une valeur de $g$ et nous observons qualitativement l'activit\u00e9 neuronale des trois populations.\n", "\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": ["net = RRNN(ring=True, recurrent=True)\n", "gs = net.g * np.logspace(-1., 1., 9)\n", "for g in gs:\n", "    fig, ax = plt.subplots(figsize=(8,8))\n", "    net.g = g\n", "    net.init_params()\n", "    df, spikesE, spikesI = net.model()\n", "    \n", "    _ = net.Raster(df, spikesE, spikesI,\n", "                   title='--------- coupling = {} ---------'.format(str(g)))\n", "\n", "    plt.show()"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": ["### Effet des bandwidths des projections internes\n", "\n", "Ici, nous \u00e9tudions l'effet de variation des diff\u00e9rentes largeurs de bande li\u00e9es aux projections internes, c'est \u00e0 dire les s\u00e9lectivit\u00e9s \u00e0 l'orientation associ\u00e9es aux diff\u00e9rentes projections, sur la s\u00e9lectivit\u00e9 du ring.\n", "\n", "Pour cela, nous fixons la bandwidth de l'entr\u00e9e \u00e0 15\u00b0 et nous g\u00e9n\u00e9rons des rasterplots pour chaque valeur de bandwidth d'une projection. Cette op\u00e9ration est r\u00e9p\u00e9t\u00e9e pour chacune des quatre projections.\n", "\n", "D'une mani\u00e8re analogue aux \u00e9tudes pr\u00e9cedentes, nous ex\u00e9cutons une simulation pour chaque valeur de bandwidth. Nous g\u00e9n\u00e9rons alors, pour chaque simulation, les rasterplots des populations source, excitatrice et inhibitrice.\n", "\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": ["bws = 180*np.logspace(0, -4, 9, base=2)\n", "#bws = 180*np.logspace(0, -4, 7, base=2)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": ["net = RRNN(ring=True, recurrent=True)\n", "net.variationRaster('b_exc_exc', bws)"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": ["net = RRNN(ring=True, recurrent=True)\n", "net.variationRaster('b_inh_exc', bws)"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": ["net = RRNN(ring=True, recurrent=True)\n", "net.variationRaster('b_inh_inh', bws)"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"execution_count": 7, "cell_type": "code", "source": ["net = RRNN(ring=True, recurrent=True)\n", "net.variationRaster('b_exc_inh', bws)"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": ["### Courbes d'accord d'un ring r\u00e9current, variation du tuning, de $W$ et de $g$\n", "\n", "Pour d\u00e9montrer notre d\u00e9marche, nous allons maintenant appliquer des entr\u00e9es s\u00e9lectives \u00e0 un r\u00e9seau de connectivit\u00e9 al\u00e9atoire et trouver la courbe de selectivit\u00e9 (de type von Mises) qui correspond \u00e0 la meilleure courbe d'accord sur les fr\u00e9quences de d\u00e9charge. Cet ajustement nous permettra de trouver un ensemble de param\u00e8tres du r\u00e9seau permettant une r\u00e9ponse robuste aux diff\u00e9rentes distributions d'orientation.\n", "\n", "Pour cela, nous simulons le ring r\u00e9current avec diff\u00e9rentes entr\u00e9es, des distributions d'orientation ayant diff\u00e9rentes largeurs de bande. Nous mesurons le taux de d\u00e9charge moyen de la population excitatrice et nous ajustons ensuite ces taux de d\u00e9charge par des distributions de Von Mises. Nous rep\u00e9tons alors ces op\u00e9rations pour diff\u00e9rentes combinaisons du tuning, du poids global $W$ et du coupling $g$."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["net = RRNN(ring=True, recurrent=True)\n", "df, spikesE, spikesI = net.model()\n", "\n", "theta, fr, result = net.fit_vonMises(spikesE)\n", "\n", "fig, ax = plt.subplots(figsize=(8,3))\n", "ax.plot(theta*180/np.pi, fr, 'bo')\n", "#plt.plot(x, result.init_fit, 'k--')\n", "ax.plot(theta*180/np.pi, result.best_fit, 'r-')\n", "ax.axis('tight')\n", "ax.set_xlabel('orientation')\n", "ax.set_ylabel('firing rate')\n", "ax.set_ylim(0)\n", "plt.show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["* fit en fonction de $W$"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["net = RRNN(ring=True, recurrent=True)\n", "ws = net.w * np.logspace(-1, 1, 7)\n", "bws = 180*np.logspace(0, -4, 5, base=2)\n", "for w in ws:\n", "    fig, ax = plt.subplots(1, 1, figsize=(8,3))\n", "    for bw in bws:\n", "        net = RRNN(ring=True, recurrent=True)\n", "        net.sim_params['b_input'] = bw\n", "        net.sim_params['b_exc_inh'] = 30.\n", "        net.sim_params['b_exc_exc'] = 5.\n", "        net.sim_params['b_inh_exc'] = 30.\n", "        net.sim_params['b_inh_inh'] = 5.\n", "\n", "        df, spikesE, spikesI = net.model()\n", "        theta, fr, result = net.fit_vonMises(spikesE)\n", "        #print(result.best_fit.mean(), result.best_fit.std())\n", "        ax.plot(theta*180/np.pi, result.best_fit, label=str(bw))\n", "\n", "    ax.set_title(' w = {}'.format(w))\n", "    ax.set_xlabel('orientation')\n", "    ax.set_ylabel('firing rate')\n", "    ax.axis('tight')\n", "    ax.set_ylim(0)\n", "    plt.legend()\n", "    plt.show()"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["for w in ws:\n", "    bws = 180*np.logspace(0, -4, 5, base=2)\n", "    fig, ax = plt.subplots(figsize=(8,3))\n", "    for bw in bws:\n", "        net = RRNN(ring=True, recurrent=True)\n", "        net.sim_params['b_input'] = bw\n", "        net.sim_params['b_exc_inh'] = 5.\n", "        net.sim_params['b_exc_exc'] = 15.\n", "        net.sim_params['b_inh_exc'] = 30.\n", "        net.sim_params['b_inh_inh'] = 15.\n", "        df, spikesE, spikesI = net.model()\n", "        theta, fr, result = net.fit_vonMises(spikesE)\n", "        #print(result.best_fit.mean(), result.best_fit.std())\n", "        ax.plot(theta*180/np.pi, result.best_fit, label=str(bw))\n", "\n", "    ax.set_title(' w = {}'.format(w))\n", "    ax.set_xlabel('orientation')\n", "    ax.set_ylabel('firing rate')\n", "    ax.axis('tight')\n", "    ax.set_ylim(0)\n", "    plt.legend()\n", "    plt.show()"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["for w in ws:\n", "    bws = 180*np.logspace(0, -4, 5, base=2)\n", "    fig, ax = plt.subplots(figsize=(8,3))\n", "    for bw in bws:\n", "        net = RRNN(ring=True, recurrent=True)\n", "        net.sim_params['b_input'] = bw\n", "        net.sim_params['b_exc_inh'] = 45.\n", "        net.sim_params['b_exc_exc'] = 5.\n", "        net.sim_params['b_inh_exc'] = 5.\n", "        net.sim_params['b_inh_inh'] = 5.\n", "        df, spikesE, spikesI = net.model()\n", "        theta, fr, result = net.fit_vonMises(spikesE)\n", "        #print(result.best_fit.mean(), result.best_fit.std())\n", "        ax.plot(theta*180/np.pi, result.best_fit, label=str(bw))\n", "\n", "    ax.set_title(' w = {}'.format(w))\n", "    ax.set_xlabel('orientation')\n", "    ax.set_ylabel('firing rate')\n", "    ax.axis('tight')\n", "    ax.set_ylim(0)\n", "    plt.legend()\n", "    plt.show()"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": ["* fit en fonction du coupling"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["net = RRNN(ring=True, recurrent=True)\n", "gs = net.g * np.logspace(-2, 2, 7)\n", "bws = 180*np.logspace(0, -4, 5, base=2)\n", "for g in gs:\n", "    fig, ax = plt.subplots(figsize=(8,3))\n", "    for bw in bws:\n", "        net = RRNN(ring=True, recurrent=True)\n", "        net.sim_params['b_input'] = bw\n", "\n", "        net.g = g\n", "        net.init_params()\n", "\n", "        df, spikesE, spikesI = net.model()\n", "        theta, fr, result = net.fit_vonMises(spikesE)\n", "        #print(result.best_fit.mean(), result.best_fit.std())\n", "        ax.plot(theta*180/np.pi, result.best_fit, label=str(bw))\n", "\n", "    ax.set_title(' g = {}'.format(g))\n", "    ax.set_xlabel('orientation')\n", "    ax.set_ylabel('firing rate')\n", "    ax.axis('tight')\n", "    ax.set_ylim(0)\n", "    plt.legend()\n", "    plt.show()"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": ["## Le ring feed-forward face au ring r\u00e9current\n", "\n", "### Diff\u00e9rences dans les motifs d'activit\u00e9\n", "\n", "Ici, nous montrons les diff\u00e9rences d'activit\u00e9 entre un ring avec une connectivit\u00e9 feed- forward et un ring poss\u00e9dant une connectivit\u00e9 r\u00e9currente. La population excitatrice d'un ring r\u00e9current bien accord\u00e9 doit repr\u00e9senter de fa\u00e7on pr\u00e9cise l'orientation soumise en entr\u00e9e m\u00eame si la distribution d'orientation \u00e0 une grande largeur de bande. \n", "\n", "Pour v\u00e9rifier cela, nous param\u00e9trons l'activit\u00e9 des neurones de la population source de telle sorte que celle-ci repr\u00e9sente une orientation de contraste de 90\u00b0 et que la largeur de bande de la distribution soumise soit de 40\u00b0. Nous g\u00e9n\u00e9rons alors les rasterplots des trois populations du ring feed-forward ainsi que du ring r\u00e9current.\n", "\n", "Si l'on s'int\u00e9resse \u00e0 l'activit\u00e9 de la population excitatrice pour chacun des deux types de ring, nous observons bien que le ring feed-forward reproduit l'activit\u00e9 de la population d'entr\u00e9e alors que le ring r\u00e9current pr\u00e9sente une activit\u00e9 plus locale. Et, puisque les neurones du ring sont organis\u00e9s selon leur pr\u00e9ference \u00e0 l'orientation, cela signifie que la connectivit\u00e9 r\u00e9currente entraine une repr\u00e9sentation de l'orientation plus pr\u00e9cise. Ainsi la r\u00e9ponse du ring recurrent est plus robuste \u00e0 l'inhomog\u00e9n\u00e9it\u00e9 de l'orientation pr\u00e9sent\u00e9e."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": ["bw = 40.\n", "markersize = .5\n", "net = RRNN(ring=True, recurrent=False)\n", "net.sim_params['b_input'] = bw\n", "net.model()\n", "title = 'Feed-Forward'\n", "\n", "fig = Figure(Panel(net.spikesP.spiketrains, xticks=False, yticks=True, ylabel=\"input\", color='k', markersize=markersize), #, line_properties=line_properties\n", "            Panel(net.spikesE.spiketrains, xticks=False, yticks=True, ylabel=\"Excitatory\", color='r', markersize=markersize),\n", "            Panel(net.spikesI.spiketrains, xlabel=\"Time (ms)\", xticks=True, yticks=True, color='b', ylabel=\"Inhibitory\", markersize=markersize),\n", "            title='--------- {} ---------'.format(title))\n", "\n", "fig.fig.savefig(\"../figs/ringFF.png\", dpi = 600)"], "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": ["net = RRNN(ring = True, recurrent=True)\n", "net.sim_params['b_input'] = bw\n", "net.model()\n", "title = 'Recurrent'\n", "\n", "fig = Figure(Panel(net.spikesP.spiketrains, xticks=False, yticks=True, ylabel=\"input\", color='k', markersize=markersize), #, line_properties=line_properties\n", "            Panel(net.spikesE.spiketrains, xticks=False, yticks=True, ylabel=\"Excitatory\", color='r', markersize=markersize),\n", "            Panel(net.spikesI.spiketrains, xlabel=\"Time (ms)\", xticks=True, yticks=True, color='b', ylabel=\"Inhibitory\", markersize=markersize),\n", "            title='--------- {} ---------'.format(title))\n", "\n", "fig.fig.savefig(\"../figs/ringRecurrent.png\", dpi = 600)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 1, "cell_type": "code", "source": ["%load_ext autoreload\n", "%autoreload 2\n", "\n", "%matplotlib inline\n", "import numpy as np\n", "from RRNN import RRNN\n", "import matplotlib.pyplot as plt"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": ["net = RRNN(ring=True, recurrent=True)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["net.sim_params['b_input']"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["bw_values = 180*np.logspace(0, -4, 5, base=2)\n", "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n", "\n", "for i in range(2):\n", "    for bw_value in bw_values:\n", "        if i==0:\n", "            net = RRNN(ring=True, recurrent=False)\n", "        else:\n", "            net = RRNN(ring=True, recurrent=True)\n", "\n", "        net.sim_params['b_input'] = bw_value\n", "        df, spikesE, spikesI = net.model()\n", "        theta, fr, result = net.fit_vonMises(spikesE)\n", "        #print(result.best_fit.mean())\n", "        axs[i].plot(theta*180/np.pi, result.best_fit, label=str(bw_value))\n", "\n", "    axs[i].set_xlabel('orientation')\n", "    axs[i].set_ylabel('firing rate')\n", "    axs[i].axis('tight')\n", "    axs[i].set_ylim([0, 30])\n", "axs[0].set_title('feed-forward ring')\n", "axs[1].set_title('recurrent ring')\n", "\n", "plt.legend(loc='best')\n", "plt.show()\n", "\n", "fig.savefig('../figs/ring_feed-forward_vs_recurrent.png', dpi = 600)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["bw_values = 180*np.logspace(-1, -10, 15, base=2)\n", "\n", "def HWHH(k):\n", "    \"\"\"\n", "    See http://motionclouds.invibe.net/posts/testing-grating.html#tuning-the-bandwidth\n", "    \n", "    \"\"\"\n", "    return .5*np.arccos(1+ np.log((1+np.exp(-2*k))/2)/k)\n", "\n", "HWHH_in = HWHH(1/np.sqrt(bw_values/180*np.pi))\n", "\n", "print(bw_values, HWHH_in)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["BW= np.zeros((2, len(bw_values)))\n", "for i in range(2):\n", "    for i_bw, bw_value in enumerate(bw_values):\n", "        if i==0:\n", "            net = RRNN(ring=True, recurrent=False)\n", "        else:\n", "            net = RRNN(ring=True, recurrent=True)\n", "\n", "        net.sim_params['b_input'] = bw_value\n", "        \n", "        df, spikesE, spikesI = net.model()\n", "        theta, fr, result = net.fit_vonMises(spikesE)\n", "        BW[i, i_bw] = result.params['sigma'].value"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n", "ax.plot(HWHH_in, HWHH(1/np.sqrt(BW[0, :])), 'k.', label='feed-forward ring')\n", "ax.plot(HWHH_in, HWHH(1/np.sqrt(BW[1, :])), 'r.', label='recurrent ring')\n", "        \n", "ax.set_xlabel('HWHH in')\n", "ax.set_ylabel('HWHH out')\n", "\n", "plt.legend(loc='best')\n", "ax.axis('tight')\n", "ax.set_xlim([0, .7])\n", "ax.set_ylim([0, .7])\n", "plt.show()\n", "\n", "fig.savefig('../figs/ring_feed-forward_vs_recurrent_BW.png', dpi = 600)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["# Discussion et perspectives \n", "\n", "## Conclusion\n", "\n", "### R\u00e9sultats obtenus\n", " \n", "Nous avons vu qu'une mod\u00e9lisation de larges r\u00e9seaux neuronaux r\u00e9sulte en fait de nombreux choix. Certains de ces choix sont faits par rapport au stimulateur et \u00e0 l'interface de programmation \u00e0 utiliser. Aussi, la grande diversit\u00e9 de mod\u00e8les neuronaux existants implique que des d\u00e9cisions sont \u00e9galement \u00e0 prendre quant au choix d'un de ces mod\u00e8les. Ensuite, nous avons expos\u00e9 le d\u00e9veloppement de l'architecture du r\u00e9seau r\u00e9current al\u00e9atoire, qui n'est rien d'autre qu'un \"ring\" d\u00e9pourvu de topologie, ainsi que son exploration. Celle-ci nous a permis d'observer quatre \u00e9tats du r\u00e9seau, d\u00e9j\u00e0 montr\u00e9s par Brunel, qui sont autant de dynamiques de son activit\u00e9 <cite data-cite=\"Brunel2000\">(Brunel, 2000)</cite>.\n", "Parmi ces quatre \u00e9tats, nous avons cherch\u00e9 \u00e0 obtenir l'\u00e9tat balanc\u00e9 en manipulant le couplage interne excitation-inhibition, $g$. Pour obtenir cet \u00e9tat, nous avons proc\u00e9d\u00e9 \u00e0 une optimisation fonctionnelle du param\u00e8tre de couplage interne, sous contraintes de coefficient de variation et de gain. Nous avons alors introduit de la topologie au sein du r\u00e9seau, pour impl\u00e9menter un ring. Et, nous avons ensuite d\u00e9but\u00e9 l'\u00e9tude de son comportement lorsqu'il est soumis \u00e0 une entr\u00e9e repr\u00e9sentant un contenu en orientations d'un stimulus visuel.\n", "\n", "Malheureusement, les r\u00e9sultats obtenus actuellement sont encore insuffisants pour effectuer toute interpr\u00e9tation ou pr\u00e9diction. Il reste encore \u00e0 observer l'effet d'une entr\u00e9e repr\u00e9sentant un contenu en orientations d'un stimulus visuel sur la r\u00e9ponse du \"ring\". Nous proc\u00e9derons alors \u00e0 un ajustement des r\u00e9sultats avec les donn\u00e9es issues des exp\u00e9rimentations physiologiques pr\u00e9vues dans le projet de recherche."], "cell_type": "markdown", "metadata": {}}, {"source": ["## Projet de th\u00e8se\n", "\n", "### Motivation et contexte\n", "\n", "La perception, dont fait partie la d\u00e9tection d'orientations, n'est qu'un versant des traitements qu'effectuent le syst\u00e8me nerveux. En effet, le syst\u00e8me nerveux permet \u00e9galement \u00e0 un individu d'agir sur son environnement, en fonction des informations sensorielles qu'il re\u00e7oit. La nature de ces informations, du moins une infime partie, a \u00e9t\u00e9 \u00e9tudi\u00e9e au cours du projet de stage gr\u00e2ce \u00e0 une approche computationnelle. Cette approche n\u00e9cessitait des mod\u00e8les de neurones avec un minimum de bior\u00e9alisme, afin de capturer les m\u00e9canismes neuronaux sous-tendant la perception de l'orientation. Nous pensons que ces mod\u00e8les sont \u00e9galement adapt\u00e9s \u00e0 une \u00e9tude des m\u00e9canismes physiologiques sous-tendant la d\u00e9cision motrice.\n", "\n", "Les d\u00e9cisions motrices sont au coeur des op\u00e9rations effectu\u00e9es par le cerveau. Comme le montrent les derni\u00e8res avanc\u00e9es en machine learning, le domaine de l'apprentissage par renforcement apporte des concepts utiles \u00e0 l'\u00e9tude des d\u00e9cisions motrices optimales.\n", "Le principe d'\u00e9chantillonage al\u00e9atoire de l'espace de r\u00e9alisation est central. En effet, lorsque le r\u00e9sultat de certaines actions est inconnu, l'\u00e9chantillonage de l'espace moteur est une bonne strat\u00e9gie. Cet \u00e9chantillonage consiste en un choix al\u00e9atoire d'actions qui va \u00eatre biais\u00e9 par l'arriv\u00e9e progressive d'informations sur l'environnement. Cette approche \u00e0 \u00e9t\u00e9 initialement d\u00e9velopp\u00e9e dans des environnements \"quadrill\u00e9s\" (comme pour le jeu d'\u00e9checs ou le jeu de go) <cite data-cite=\"sutton\"></cite>, cependant la g\u00e9n\u00e9ralisation de cette strat\u00e9gie \u00e0 des environnements plus complexes, o\u00f9 les associations situation-action sont continues, est maintenant chose courante dans le domaine de la robotique et du contr\u00f4le. Des sch\u00e9mas sensori-moteurs efficaces peuvent \u00eatre appris de cette mani\u00e8re dans le cas o\u00f9 l'environnement se constitue de cibles statiques ou mobiles  <cite data-cite=\"dauce\"></cite>, et diverses impl\u00e9mentations neuromim\u00e9tiques ont \u00e9t\u00e9 propos\u00e9es ces derni\u00e8res ann\u00e9es  <cite data-cite=\"florian\"></cite><cite data-cite=\"dauce2\"></cite>. N\u00e9ammoins, beaucoup de probl\u00e8mes restent \u00e0 r\u00e9soudre pour comprendre l'ensemble du flux d'actions, des premiers traitements sensoriels jusqu'\u00e0 l'ex\u00e9cution motrice finale. Aussi, la plupart des th\u00e9ories ont \u00e9chou\u00e9es \u00e0 expliquer l'adaptabilit\u00e9 (i.e. la g\u00e9n\u00e9ralisation d'une t\u00e2che) des syst\u00e8mes sensori-moteurs biologiques.\n", "En effet, malgr\u00e9 des ann\u00e9es d'\u00e9tudes, beaucoup de mod\u00e8les de d\u00e9cision propos\u00e9s dans la litt\u00e9rature restent \u00e9vasifs sur le substrat neuronal des m\u00e9canismes d'adaptation des d\u00e9cisions motrices.\n", "\n", "Deux approches sont g\u00e9n\u00e9ralement utilis\u00e9es pour \u00e9tudier les processus de d\u00e9cision sensori-motrice en Neurosciences : la premi\u00e8re se concentre sur les t\u00e2ches de cat\u00e9gorisation perceptive discr\u00e8te (2AFC notamment) et un Drift-Diffusion-Model (DDM) qui postule l'existence d'une accumulation d'\u00e9vidences sensorielles bruit\u00e9es jusqu'\u00e0 qu'un seuil soit atteint. Ces mod\u00e8les permettent la pr\u00e9diction de distributions de pr\u00e9cision de r\u00e9ponse et de temps de r\u00e9action (RT). Cependant, les DDMs sont des mod\u00e8les ad hoc descriptifs et ne peuvent pas rendre compte de d\u00e9cisions plus complexes. La seconde approche traite traditionnellement de t\u00e2ches de contr\u00f4le moteur continu. Dans ce flux continu d'informations sensorielles, le cerveau permet des comportements contrast\u00e9s en fonction de diff\u00e9rentes \u00e9chelles temporelles, du simple r\u00e9flexe \u00e0 un apprentissage et une prise de d\u00e9cision \u00e9labor\u00e9e. Par exemple, la vision d'un objet mobile peut donner lieu \u00e0 une vari\u00e9t\u00e9 de d\u00e9cisions motrices telles que la pr\u00e9hension ou l'esquive, selon la nature et la trajectoire de l'objet.\n", "\n", "Dans ce contexte, l'\u00e9tude des mouvements oculaires permet de mettre \u00e0 l'\u00e9preuve les mod\u00e8les modernes de d\u00e9cision et d'adaptation motrice. L'orientation visuelle (i.e. le choix de l'endroit o\u00f9 le regard se porte), r\u00e9alis\u00e9e de nombreuses fois par minute chez l'homme et l'animal, est effectivement un des actes moteurs les plus \u00e9l\u00e9mentaires. Ce type de mouvement a \u00e9t\u00e9 tr\u00e9s \u00e9tudi\u00e9 au cours des cinquante derni\u00e8res ann\u00e9es, et le but de ce projet est de mettre au d\u00e9fi les concepts cl\u00e9s de l'apprentissage par renforcement, en utilisant une large connaissance de l'exp\u00e9rimentation et de la mod\u00e9lisation portant sur les mouvements oculaires. R\u00e9cemment, des chercheurs  ont mod\u00e9lis\u00e9 ce type de t\u00e2che \u00e0 l'aide d'inf\u00e9rences Bay\u00e9siennes . Ces derni\u00e8res repr\u00e9sentent explicitement les dynamiques des croyances de l'individu sur sa relation au monde et, ainsi, elles peuvent rendre compte des m\u00e9canismes de l'adaptation motrice, ce qui manquait dans les pr\u00e9c\u00e9dents travaux <cite data-cite=\"montagnini\"></cite><cite data-cite=\"perrinet\"></cite><cite data-cite=\"orban\"></cite><cite data-cite=\"daye\"></cite>. \n", "\n", "\n", "![R\u00e9seau de neurones organis\u00e9 en \"ring\" pour la detection d'une direction d'un regard.](/tmp/future_model.png)\n", "\n", "### Th\u00e9orie\n", "\n", "Notre but est de cr\u00e9er une base \u00e0 une compr\u00e9hension globale d'une simple adaptation sensorimotrice. L'approche de mod\u00e9lisation sera bas\u00e9e sur le paradigme de minimisation de l'\u00e9nergie libre d\u00e9velopp\u00e9e par Karl Friston, lequel s'adapte bien \u00e0 l'\u00e9tude des mouvements oculaires <cite data-cite=\"perrinet\"></cite>. Cette approche h\u00e9rite et \u00e9tend de pr\u00e9c\u00e9dentes formalisations, telles que le filtre de Kalman ou le filtrage bay\u00e9sien variationnel, par une formalisation de principe de la surprise dans le syst\u00e8me \u00e9tudi\u00e9. Ces derni\u00e8res ann\u00e9es, les approches de la perception visuelle, par le principe d'\u00e9nergie libre (FEP pour Free Energy principle), ont donn\u00e9 naissance \u00e0 un cadre de travail unifi\u00e9 pour le d\u00e9veloppement de mod\u00e8les de d\u00e9cision sensorimotrice, parmi lesquels :\n", "\n", "1) L'exploration visuelle : Le FEP consid\u00e8re les mouvements oculaires comme un sondage de l'environnement <cite data-cite=\"friston\"></cite>. Ainsi, l'exploration par le biais de la saccade contribue \u00e0 la construction d'une vision int\u00e9gr\u00e9e de l'espace et des objets environnants;\n", "\n", "2) La pr\u00e9diction : id\u00e9alement, la pr\u00e9cision des mouvements est maintenue en d\u00e9pit de changements survenant dans l'environnement ou \u00e0 l'int\u00e9rieur de l'organisme. L'acte de vision, par exemple, \u00e9quivaut \u00e0 constamment extraire de nouvelles informations, venant de zones non fiables, ou variables, de la sc\u00e8ne visuelle;\n", "\n", "3) L'apprentissage qui est le changement des relations \u00e9l\u00e9mentaires entre la perception et l'action, en ce qui concerne les attentes \u00e0 propos d'objets de l'environnement.\n", "\n", "Ainsi, diff\u00e9rents aspects des mouvements oculaires sont bien expliqu\u00e9s par le codage pr\u00e9dictif, une approche du FEP. Id\u00e9alement, \u00e9quiper un syst\u00e8me (optimis\u00e9 gr\u00e2ce au FEP) de moyens de produire une action sur son environnement, lui offre une nouvelle mani\u00e8re d'\u00e9chantilloner son espace visuel. Ce type de strat\u00e9gie est appel\u00e9e le paradigme d'inf\u00e9rence active. Il a \u00e9t\u00e9 prouv\u00e9 que celui-ci est hautement g\u00e9n\u00e9ralisable et biologiquement plausible <cite data-cite=\"perrinet\"></cite>. Cependant ce paradigme n'a pas \u00e9t\u00e9 mis \u00e0 l'\u00e9preuve dans la compr\u00e9hension de l'adaptation motrice, notamment concernant le syst\u00e8me oculomoteur.\n", "\n", "### M\u00e9thodes\n", "\n", "L'originalit\u00e9 du projet repose sur l'\u00e9tude de mouvements oculaires volontaires, en r\u00e9ponse \u00e0 des informations visuelles, qui constitue un mod\u00e8le d'apprentissage moteur utilisant le paradigme d'inf\u00e9rence active. Des donn\u00e9es issues d'exp\u00e9riences comportementales conduites chez l'Homme seront utilis\u00e9es pour tester le mod\u00e8le. Ces exp\u00e9riences portaient principalement sur les modulations du comportement oculomoteur, comme la latence, l'amplitude ou la vitesse, observ\u00e9es dans un environnement changeant, en relation avec un protocole de renforcement dynamique. Ce jeu de donn\u00e9es fournit un banc de test id\u00e9al pour sonder les d\u00e9cisions sensorimotrices ayant lieu sur diff\u00e9rentes \u00e9chelles temporelles, sur plusieurs niveaux de traitement et sur de nombreux mouvements (e.g. poursuites versus saccades <cite data-cite=\"orban\"></cite><cite data-cite=\"daye\"></cite><cite data-cite=\"fleuriet\"></cite>). De plus, la manipulation du d\u00e9lai <cite data-cite=\"perrinet\"></cite>, de l'attente sensorielle ou encore de la cons\u00e9quence associ\u00e9e aux r\u00e9ponses motrices r\u00e9v\u00e8le une remarquable flexibilit\u00e9 des comportements oculomoteurs <cite data-cite=\"madelain\"></cite>.\n", "\n", "### R\u00e9sultats attendus\n", "  \n", "Les d\u00e9cisions sensorimotrices se d\u00e9clinent sur diff\u00e9rentes \u00e9chelles temporelles, de dizaines de secondes \u00e0 des heures, voire des jours (i.e. \u00e9chelles temporelles d'adaptation et apprentissage), selon le type de prise de d\u00e9cision et le r\u00e9pertoire de mouvement. En particulier, bien que le DDM standard admet qu'une d\u00e9cision motrice soit prise apr\u00e8s qu'un seuil soit franchi, aucune pr\u00e9diction n'est faite \u00e0 propos : 1) du contr\u00f4le du mouvement (i.e. sur le fait qu'il soit ou non modul\u00e9 par des informations sensorielles); 2) de l'effecteur concern\u00e9 (oeil ou main); 3) de la valeur de mouvement (succ\u00e8s ou \u00e9chec, b\u00e9n\u00e9fique ou d\u00e9l\u00e9t\u00e8re, etc...);\n", "\n", "Nous allons donc \u00e9tudier et mod\u00e9liser ces diff\u00e9rentes variables du traitement de la d\u00e9cision. Ceci permet d'identifier trois axes majeurs dans le d\u00e9veloppement du projet:\n", "\n", "1) Exploration de l'espace visuel : identifier les \u00e9chelles temporelles caract\u00e9ristiques sur lesquelles la d\u00e9cision motrice est faite. Les mod\u00e8les biologiquement inspir\u00e9s de l'activit\u00e9 de population <cite data-cite=\"montagnini\"></cite><cite data-cite=\"perrinet\"></cite> et de la plasticit\u00e9 <cite data-cite=\"dauce\"></cite> seront confront\u00e9s \u00e0 des donn\u00e9es comportementales existantes sur l'exploration de l'espace visuel <cite data-cite=\"friston\"></cite>. En particulier, la strat\u00e9gie d'exploration al\u00e9atoire uniforme devra \u00eatre compar\u00e9e \u00e0 des strat\u00e9gies d'exploration non al\u00e9atoire ainsi que des hypoth\u00e8ses \"curiosity-based\".\n", "2) Dynamique du changement de d\u00e9cision et d'action. Les t\u00e2ches visuo-motrices sont des bancs d'essai classiques pour \u00e9tudier le choix cat\u00e9goriel, mais peu d'aspects de la dynamique du changement dans les r\u00e9ponses motrices sont connus. Quelle est la rapidit\u00e9 avec laquelle les sujets modifient un sch\u00e9ma moteur lorsque les conditions environnementales \u00e9voluent? Est-ce que les sujets d\u00e9tectent les changements? Vont-ils continuer leur action ou l'adapter aux nouvelles conditions? Afin de mieux pr\u00e9dire les s\u00e9ries caract\u00e9ristiques du changement de la r\u00e9ponse, face \u00e0 des situations ambig\u00fces et/ou inconstantes, nous testerons le r\u00f4le pr\u00e9dictif des fluctuations al\u00e9atoires de la r\u00e9ponse en relation avec la dynamique d'adaptation <cite data-cite=\"Leon12\"></cite>. Une importance particuli\u00e8re sera donn\u00e9e: 2a) \u00e0 l'analyse de la combinaison dynamique des mouvements de poursuites et de saccades dans la poursuite d'un objet mobile en cas d'incertitude sensorielle \u00e0 propos de la position, de la direction et la vitesse de la cible; 2b) aux dynamiques de l'adaptation saccadique dans des protocoles \u00e0 double-\u00e9tape.\n", "3) Apprentissage et adaptation sur le long terme. Les erreurs de pr\u00e9diction et les corrections en cours de mouvement sont des fonctionnalit\u00e9s essentielles des traitements li\u00e9s \u00e0 la d\u00e9cision. Les mod\u00e8les error-based de d\u00e9cision en cas d'incertitude existent chez l'humain et l'animal et permettent une am\u00e9lioration des temps de r\u00e9action et de la pr\u00e9cision du mouvement.  \n", "\n", "Le cas de la capture visuelle peut, par exemple, sugg\u00e9rer un principe bas\u00e9 sur la r\u00e9compense dans la fov\u00e9ation de cibles visuelles saillantes. Nous allons devoir comparer un simple apprentissage non supervis\u00e9 d'invariances sensorimotrices, avec un apprentissage moteur bas\u00e9 sur la r\u00e9compense, qui prescrit implicitement un choix optimal d'action \u00e0 effectuer dans le contexte visuel. En \u00e9tudiant simultanemment diff\u00e9rents types de mouvement oculaire (poursuite, poursuite lisse et saccade) sous contraintes de pr\u00e9cision, nous cherchons \u00e0 mettre au d\u00e9fi la capacit\u00e9 adaptative des mod\u00e8les de d\u00e9cision visuomotrice. Un accent sera mis sur la conception et le test d'un mod\u00e8le inf\u00e9rentiel pr\u00e9disant des distributions de temps de r\u00e9action o\u00f9 interviennent des effets d'adaptation \u00e0 long terme.\n", "\n", "L'adaptation des d\u00e9cision motrices est un vaste champ de recherche et nous esp\u00e9rons y contribuer de fa\u00e7on originale. Ici, nous confronterons les avanc\u00e9es les plus r\u00e9centes de la mod\u00e9lisation de l'apprentissage par renforcement, avec des observations quantitatives des op\u00e9rations visuelles effectu\u00e9es par les sujets humains et animaux, en utilisant des outils modernes d'observation. Nous nous attendons \u00e0 ce que ce projet permette une meilleure compr\u00e9hension des processus de d\u00e9cision chez les sujets sains et les patients atteints, par exemple, d'autisme ou de schizophr\u00e9nie <cite data-cite=\"adams\"></cite>. Cette recherche fondamentale pourrait donner lieu \u00e0 des applications dans des domaines tels que la robotique ou la recherche clinique."], "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "widgets": {"state": {}, "version": "1.1.2"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.5.2", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}